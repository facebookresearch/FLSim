{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "last_server_session_id": "",
    "last_kernel_id": "",
    "last_base_url": "",
    "last_msg_id": "",
    "outputWidgetContext": {},
    "colab": {
      "name": "cifar10_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "ec4510e7-5bb1-45a2-ac60-57ca1906cf03",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "RcL6FFVmO_we"
      },
      "source": [
        "# FLSim Tutorial: Image classification with CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "14df5562-53c7-42ee-bafa-a5bcc0e32826",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "9gIQhqPSO_wg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will train a simple CNN image classifier on CIFAR-10 with federated learning using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "a4acb953-0669-4c3f-8ce9-4d798c87ca7a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "qCwSd63HO_wg"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "To get the most of this tutorial, you should be comfortable with training machine learning models with **PyTorch** and familiar with the concept of **federated learning (FL)**. If you are unfamiliar with either of them or could use a refresher, please take a look at the following resources before proceeding with the tutorial:\n",
        "\n",
        "- McMahan & Ramage (2017): [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html). A short blog post from Google AI introducing the main idea of FL in a beginner-friendly way.\n",
        "- McMahan et al. (2017): [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/pdf/1602.05629.pdf). This paper first proposes the approach of federated learning. The described algorithm is now known as federated averaging (or FedAvg for short).\n",
        "- PyTorch has [extensive tutorials](https://pytorch.org/tutorials/) on their website. In particular, take a look at their [image classification tutorial using CIFAR-10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
        "\n",
        "Now that you're familiar with PyTorch and FL, let's move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "eeb9679b-564f-41f4-ac5e-68af58d18d0b",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "90vb3wIbO_wg"
      },
      "source": [
        "### Objectives \n",
        "\n",
        "By the end of this tutorial, we will have learnt how to\n",
        "\n",
        "1. Build a data pipeline for federated learning with FLSim,\n",
        "2. Create an image classification model compatible with FL training,\n",
        "3. Create a metrics reporter to collect and report metrics,\n",
        "4. Set hyperparameters for FL training, and\n",
        "5. Launch an FL training flow using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "b82a213b-1dc9-4d0b-be5f-24f7341f25bd",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "SEk7EMBXO_wh"
      },
      "source": [
        "## Training an image classifier with FLSim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-rYy6_w7kT"
      },
      "source": [
        "### Prerequisites\n",
        "First, let us install flsim via pip with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdMaaGUWjWbs"
      },
      "source": [
        "!pip install --quiet flsim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ5ByjeMedhi"
      },
      "source": [
        "Some useful parameters for later - no need to change these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAWKDKuieYPO"
      },
      "source": [
        "USE_CUDA = True\n",
        "LOCAL_BATCH_SIZE = 32\n",
        "EXAMPLES_PER_USER = 500\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "# suppress large outputs\n",
        "VERBOSE = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "e78c1595-1416-4a95-8ace-e281555b052a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "2DgQGgN0O_wi"
      },
      "source": [
        "### 0. About the dataset\n",
        "\n",
        "For this tutorial, we will use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR-10 dataset consists of 60k 3x32x32 3-channel color images with 32x32 pixels from 10 classes, with 6k images per class. \n",
        "There are 50k training images (5k training images per class) and 10k test images (1k test images per class).\n",
        "The classes are ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, and ‘truck’.\n",
        "\n",
        "![img](https://pytorch.org/tutorials/_images/cifar10.png)\n",
        "\n",
        "We can get the CIFAR-10 dataset from `torchvision.datasets`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "f419c741-d171-4d8a-b731-4ae61916945c",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "f419c741-d171-4d8a-b731-4ae61916945c",
        "executionStartTime": 1636504268213,
        "executionStopTime": 1636504272232,
        "id": "ii-IOAM6O_wi"
      },
      "source": [
        "from torchvision.datasets.cifar import CIFAR10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "f476eb9e-a006-4cbc-81e2-585e771e6707",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "-NipUzoIO_wi"
      },
      "source": [
        "### 1. Data pipeline\n",
        "\n",
        "First, let's define how to build the data pipeline for federated learning:\n",
        "\n",
        "1. We create data transforms and training, eval, and test datasets. This step is identical to preparing data in non-federated learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE8WbhrhoYLk",
        "outputId": "bf447807-58c3-48f3-84c3-e833ec02629c"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# 1. Create training, eval, and test datasets like in non-federated learning.\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.4914, 0.4822, 0.4465), \n",
        "            (0.2023, 0.1994, 0.2010)\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "train_dataset = CIFAR10(\n",
        "    root=\"./cifar10\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = CIFAR10(\n",
        "    root=\"./cifar10\", train=False, download=True, transform=transform\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk0ckcDsoX8g"
      },
      "source": [
        "\n",
        "There are a few extra steps to enable training with federated learning. In particular, we need to\n",
        "\n",
        "2. Create a sharder, which defines a mapping from examples in the training data to clients. In other words, a sharder groups rows of data into client datasets and returns a list of list of examples. FLSim provides a number of sharding strategies such as random or column-based sharding. \n",
        "In this tutorial, we use sequential sharding, which assigns the first `examples_per_user` rows to user 0, the second `examples_per_user` rows to user 1, etc. \n",
        "\n",
        "3. Create a data loader, which will shard and batchify training, eval, and test data. For each dataset, the data loader first assigns rows to clients using the sharder and then splits each client's data into batches of size `batch_size`. We choose not to drop the last batch.\n",
        "\n",
        "4. Lastly, wrap the data loader with a data provider and return it. The data provider creates clients from the groupings in the data loader and adds metadata (e.g. number of examples/batches). Our data is now formatted such that the trainer will accept it.\n",
        "\n",
        "Note that the concept of a client or device only applies to the training data, the eval and test set data are identical to non-federated learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljDjx6CRotTW",
        "outputId": "782521e0-dead-4a8a-ae90-fe61cc8c5cd9"
      },
      "source": [
        "from flsim.data.data_sharder import SequentialSharder\n",
        "from flsim.utils.example_utils import DataLoader, DataProvider\n",
        "\n",
        "# 2. Create a sharder, which maps samples in the training data to clients.\n",
        "sharder = SequentialSharder(examples_per_shard=EXAMPLES_PER_USER)\n",
        "\n",
        "# 3. Shard and batchify training, eval, and test data.\n",
        "fl_data_loader = DataLoader(\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    sharder=sharder,\n",
        "    batch_size=LOCAL_BATCH_SIZE,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# 4. Wrap the data loader with a data provider.\n",
        "data_provider = DataProvider(fl_data_loader)\n",
        "print(f\"\\nClients in total: {data_provider.num_train_users()}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating FL User: 100user [00:15,  6.60user/s]\n",
            "Creating FL User: 20user [00:02,  6.93user/s]\n",
            "Creating FL User: 20user [00:02,  6.98user/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clients in total: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "06d24749-da10-4a1b-a48d-3810fda4ebf9",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "tpZ2H7jNO_wk"
      },
      "source": [
        "### 2. Create the model\n",
        "\n",
        "Now, let's see how we can create a model that is compatible with FL-training.\n",
        "\n",
        "1. First, we define a standard, non-FL image classification PyTorch `nn.Module.` In this tutorial we use a simple CNN with 4 convolutional layers, a group norm, and a linear layer. \n",
        "\n",
        "2. Create a `torch.device` and choose where the model will be allocated (CUDA or CPU).\n",
        "\n",
        "As with the data pipeline, these steps are identical to creating a model in non-federated learning. Note that in contrast to non-FL learning, we haven't moved the model to device yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7d1549e0-5da1-4480-b581-0c46747a301d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "7d1549e0-5da1-4480-b581-0c46747a301d",
        "executionStartTime": 1636422569684,
        "executionStopTime": 1636422570122,
        "id": "ktgmrbTyO_wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffdb2f8-3a55-4aff-b616-c904adbdd602"
      },
      "source": [
        "import torch\n",
        "from flsim.utils.example_utils import SimpleConvNet\n",
        "\n",
        "# 1. Define our model, a simple CNN.\n",
        "model = SimpleConvNet(in_channels=3, num_classes=10)\n",
        "\n",
        "# 2. Choose where the model will be allocated.\n",
        "cuda_enabled = torch.cuda.is_available() and USE_CUDA\n",
        "device = torch.device(f\"cuda:{0}\" if cuda_enabled else \"cpu\")\n",
        "\n",
        "model, device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(SimpleConvNet(\n",
              "   (layers): ModuleList(\n",
              "     (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "     (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "     (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "     (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "   )\n",
              "   (gn_relu): Sequential(\n",
              "     (0): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
              "     (1): ReLU()\n",
              "     (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   )\n",
              "   (dropout): Dropout(p=0, inplace=False)\n",
              "   (fc): Linear(in_features=288, out_features=10, bias=True)\n",
              " ), device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tka3JhCyDx"
      },
      "source": [
        "As with the data pipeline, there are a few extra steps that we need to take to make sure that our model is compatible with FL. In particular, we need to\n",
        "\n",
        "3. Wrap the PyTorch module with the FLSim `FLModel`, an abstracted version of a FL-friendly model class that is accepted by the trainer and handles metric collection, as well as the forward pass for both training and evaluation. We can recover our `nn.Module` by calling `FLModel.fl_get_module()`\n",
        "\n",
        "4. Move the model to GPU and enable CUDA if desired. `FLModel.fl_cuda()` internally calls `model.to(device)` to move the model to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "fa53c62c-7732-4f31-b73a-3fd61a9ac6e0",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "fa53c62c-7732-4f31-b73a-3fd61a9ac6e0",
        "executionStartTime": 1636422570217,
        "executionStopTime": 1636422570544,
        "id": "i1wVfw3OO_wl"
      },
      "source": [
        "from flsim.utils.example_utils import FLModel\n",
        "\n",
        "# 3. Wrap the model with FLModel.\n",
        "global_model = FLModel(model, device)\n",
        "assert(global_model.fl_get_module() == model)\n",
        "\n",
        "# 4. Move the model to GPU and enable CUDA if desired.\n",
        "if cuda_enabled:\n",
        "    global_model.fl_cuda()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJETpvZtDP0O"
      },
      "source": [
        "### 3. Metrics Reporting\n",
        "\n",
        "After having created our data pipeline and FL model, we will now create our metrics reporter. \n",
        "The metrics reporter allows us to collect, evaluate, and report relevant training, aggregation, and evaluation/test metrics as well as log them onto TensorBoard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3dLLIn7ExUq"
      },
      "source": [
        "from flsim.interfaces.metrics_reporter import Channel\n",
        "from flsim.utils.example_utils import MetricsReporter\n",
        "\n",
        "# Create a metric reporter.\n",
        "metrics_reporter = MetricsReporter([Channel.TENSORBOARD, Channel.STDOUT])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO3I_1GF6xzH"
      },
      "source": [
        "There are three functions that are of particular interest:\n",
        "\n",
        "1. `compute_scores` computes the metrics of interest for both training and aggregation (if desired) as well as evaluation/test.\n",
        "\n",
        "2. `create_eval_metrics` creates a dictionary that stores the value for each eval metric. \n",
        "\n",
        "3. `compare_metrics` compares the current eval metrics that are returned by `create_eval_metrics` to the best eval metrics so far.\n",
        "\n",
        "\n",
        "For this tutorial, our only metric of interest is top-1 accuracy. In general, as with the data loading and model, you should write your own metrics reporter depending on the task. For example, if you are running an NLP task you may want to have your metrics reporter track perplexity as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfuzBfS76yGZ"
      },
      "source": [
        "import inspect\n",
        "\n",
        "if VERBOSE:\n",
        "    print(inspect.getsource(MetricsReporter.compute_scores))\n",
        "    print(inspect.getsource(MetricsReporter.create_eval_metrics))\n",
        "    print(inspect.getsource(MetricsReporter.compare_metrics))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "983d685b-8e16-4efa-8a10-ec4ab73a69bc",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "ZmvB2-aNO_wl"
      },
      "source": [
        "### 4. Hyperparameters\n",
        "\n",
        "We can represent the hyperparameters for FL training in a JSON config for ease of representation and we convert the JSON config to OmegaConf before passing it to the FL trainer.\n",
        "\n",
        "In particular, we specify a FedAvg implementation with 10 users per round."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "3cdafa88-21b4-4080-ba20-2890431b32dd",
        "showInput": true,
        "customInput": null,
        "requestMsgId": "3cdafa88-21b4-4080-ba20-2890431b32dd",
        "executionStartTime": 1636422570651,
        "executionStopTime": 1636422571503,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "nMCgbRFEO_wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac845a9e-df21-4c94-cf3c-667c111f9bbb"
      },
      "source": [
        "import flsim.configs\n",
        "from flsim.utils.config_utils import fl_config_from_json\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "json_config = {\n",
        "    \"trainer\": {\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        # there are different types of aggregator\n",
        "        # fed avg doesn't require lr, while others such as fed_avg_with_lr or fed_adam do\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        \"server\": {\n",
        "            \"_base_\": \"base_sync_server\",\n",
        "            \"server_optimizer\": {\n",
        "                \"_base_\": \"base_fed_avg_with_lr\",\n",
        "                \"lr\": 2.13,\n",
        "                \"momentum\": 0.9\n",
        "            },\n",
        "            # type of user selection sampling\n",
        "            \"active_user_selector\": {\"_base_\": \"base_uniformly_random_active_user_selector\"},\n",
        "        },\n",
        "        \"client\": {\n",
        "            # number of client's local epoch\n",
        "            \"epochs\": 1,\n",
        "            \"optimizer\": {\n",
        "                \"_base_\": \"base_optimizer_sgd\",\n",
        "                # client's local learning rate\n",
        "                \"lr\": 0.01,\n",
        "                # client's local momentum\n",
        "                \"momentum\": 0,\n",
        "            },\n",
        "        },\n",
        "        # number of users per round for aggregation\n",
        "        \"users_per_round\": 5,\n",
        "        # total number of global epochs\n",
        "        # total #rounds = ceil(total_users / users_per_round) * epochs\n",
        "        \"epochs\": 1,\n",
        "        # frequency of reporting train metrics\n",
        "        \"train_metrics_reported_per_epoch\": 100,\n",
        "        # frequency of evaluation per epoch\n",
        "        \"eval_epoch_frequency\": 1,\n",
        "        \"do_eval\": True,\n",
        "        # should we report train metrics after global aggregation\n",
        "        \"report_train_metrics_after_aggregation\": True,\n",
        "    }\n",
        "}\n",
        "cfg = fl_config_from_json(json_config)\n",
        "if VERBOSE: print(OmegaConf.to_yaml(cfg))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  message=\"hydra.experimental.compose() is no longer experimental.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d3296d38-38da-44ea-ad8e-305de0c3e06d",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "GBvhMLL1O_wm"
      },
      "source": [
        "### 5. Training\n",
        "Recall that we already built the data provider and created a model compatible with FL training. \n",
        "We also initialized a metrics reporter and set our desired hyperparameters.\n",
        "\n",
        "Now, we only need to instantiate the trainer with the model and hyperparameter config we defined earlier to launch the FL training flow. We run FL training with the above JSON config and utilize `eval_score` to store the final evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2eccb04f-229a-43f9-8310-0ced64132e2b",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "2eccb04f-229a-43f9-8310-0ced64132e2b",
        "executionStartTime": 1636422644878,
        "executionStopTime": 1636422645173,
        "id": "kLWIpCxjO_wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d344cfe0-60bd-475f-f57e-226621639d03"
      },
      "source": [
        "from hydra.utils import instantiate\n",
        "\n",
        "# Instantiate the trainer.\n",
        "trainer = instantiate(cfg.trainer, model=global_model, cuda_enabled=cuda_enabled)   \n",
        "\n",
        "# Launch FL training.\n",
        "final_model, eval_score = trainer.train(\n",
        "    data_provider=data_provider,\n",
        "    metrics_reporter=metrics_reporter,\n",
        "    num_total_users=data_provider.num_train_users(),\n",
        "    distributed_world_size=1\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Round:   0%|          | 0/20 [00:00<?, ?round/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train finished Global Round: 1\n",
            "(epoch = 1, round = 1, global round = 1), Loss/Training: 2.3135810285806655\n",
            "(epoch = 1, round = 1, global round = 1), Accuracy/Training: 15.0\n",
            "reporting (epoch = 1, round = 1, global round = 1) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:   5%|▌         | 1/20 [00:06<02:02,  6.45s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 1, global round = 1), Loss/Aggregation: 2.191663905978203\n",
            "(epoch = 1, round = 1, global round = 1), Accuracy/Aggregation: 16.72\n",
            "Train finished Global Round: 2\n",
            "(epoch = 1, round = 2, global round = 2), Loss/Training: 2.09371999502182\n",
            "(epoch = 1, round = 2, global round = 2), Accuracy/Training: 23.28\n",
            "reporting (epoch = 1, round = 2, global round = 2) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  10%|█         | 2/20 [00:12<01:54,  6.38s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 2, global round = 2), Loss/Aggregation: 1.971905717253685\n",
            "(epoch = 1, round = 2, global round = 2), Accuracy/Aggregation: 28.88\n",
            "Train finished Global Round: 3\n",
            "(epoch = 1, round = 3, global round = 3), Loss/Training: 1.9661175429821014\n",
            "(epoch = 1, round = 3, global round = 3), Accuracy/Training: 29.16\n",
            "reporting (epoch = 1, round = 3, global round = 3) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  15%|█▌        | 3/20 [00:19<01:48,  6.39s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 3, global round = 3), Loss/Aggregation: 1.9884823530912399\n",
            "(epoch = 1, round = 3, global round = 3), Accuracy/Aggregation: 25.64\n",
            "Train finished Global Round: 4\n",
            "(epoch = 1, round = 4, global round = 4), Loss/Training: 1.8890881791710854\n",
            "(epoch = 1, round = 4, global round = 4), Accuracy/Training: 32.76\n",
            "reporting (epoch = 1, round = 4, global round = 4) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  20%|██        | 4/20 [00:25<01:41,  6.35s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 4, global round = 4), Loss/Aggregation: 1.8473846212029457\n",
            "(epoch = 1, round = 4, global round = 4), Accuracy/Aggregation: 33.4\n",
            "Train finished Global Round: 5\n",
            "(epoch = 1, round = 5, global round = 5), Loss/Training: 1.7667964115738868\n",
            "(epoch = 1, round = 5, global round = 5), Accuracy/Training: 37.64\n",
            "reporting (epoch = 1, round = 5, global round = 5) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  25%|██▌       | 5/20 [00:31<01:34,  6.32s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 5, global round = 5), Loss/Aggregation: 1.7339487180113793\n",
            "(epoch = 1, round = 5, global round = 5), Accuracy/Aggregation: 38.4\n",
            "Train finished Global Round: 6\n",
            "(epoch = 1, round = 6, global round = 6), Loss/Training: 1.6374926581978797\n",
            "(epoch = 1, round = 6, global round = 6), Accuracy/Training: 41.52\n",
            "reporting (epoch = 1, round = 6, global round = 6) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  30%|███       | 6/20 [00:38<01:28,  6.33s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 6, global round = 6), Loss/Aggregation: 1.6723702892661094\n",
            "(epoch = 1, round = 6, global round = 6), Accuracy/Aggregation: 39.36\n",
            "Train finished Global Round: 7\n",
            "(epoch = 1, round = 7, global round = 7), Loss/Training: 1.6448417752981186\n",
            "(epoch = 1, round = 7, global round = 7), Accuracy/Training: 41.72\n",
            "reporting (epoch = 1, round = 7, global round = 7) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  35%|███▌      | 7/20 [00:44<01:22,  6.32s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 7, global round = 7), Loss/Aggregation: 1.7339074447751046\n",
            "(epoch = 1, round = 7, global round = 7), Accuracy/Aggregation: 37.84\n",
            "Train finished Global Round: 8\n",
            "(epoch = 1, round = 8, global round = 8), Loss/Training: 1.6264008715748788\n",
            "(epoch = 1, round = 8, global round = 8), Accuracy/Training: 42.04\n",
            "reporting (epoch = 1, round = 8, global round = 8) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  40%|████      | 8/20 [00:50<01:15,  6.32s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 8, global round = 8), Loss/Aggregation: 1.6474411085247993\n",
            "(epoch = 1, round = 8, global round = 8), Accuracy/Aggregation: 42.12\n",
            "Train finished Global Round: 9\n",
            "(epoch = 1, round = 9, global round = 9), Loss/Training: 1.5776848256587983\n",
            "(epoch = 1, round = 9, global round = 9), Accuracy/Training: 43.76\n",
            "reporting (epoch = 1, round = 9, global round = 9) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  45%|████▌     | 9/20 [00:56<01:09,  6.31s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 9, global round = 9), Loss/Aggregation: 1.6118090942502021\n",
            "(epoch = 1, round = 9, global round = 9), Accuracy/Aggregation: 41.16\n",
            "Train finished Global Round: 10\n",
            "(epoch = 1, round = 10, global round = 10), Loss/Training: 1.5338441893458366\n",
            "(epoch = 1, round = 10, global round = 10), Accuracy/Training: 44.96\n",
            "reporting (epoch = 1, round = 10, global round = 10) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  50%|█████     | 10/20 [01:03<01:03,  6.32s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 10, global round = 10), Loss/Aggregation: 1.6448743045330048\n",
            "(epoch = 1, round = 10, global round = 10), Accuracy/Aggregation: 40.68\n",
            "Train finished Global Round: 11\n",
            "(epoch = 1, round = 11, global round = 11), Loss/Training: 1.5130361467599869\n",
            "(epoch = 1, round = 11, global round = 11), Accuracy/Training: 46.28\n",
            "reporting (epoch = 1, round = 11, global round = 11) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  55%|█████▌    | 11/20 [01:09<00:56,  6.31s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 11, global round = 11), Loss/Aggregation: 1.6187414556741715\n",
            "(epoch = 1, round = 11, global round = 11), Accuracy/Aggregation: 41.04\n",
            "Train finished Global Round: 12\n",
            "(epoch = 1, round = 12, global round = 12), Loss/Training: 1.480795369297266\n",
            "(epoch = 1, round = 12, global round = 12), Accuracy/Training: 47.12\n",
            "reporting (epoch = 1, round = 12, global round = 12) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  60%|██████    | 12/20 [01:15<00:50,  6.30s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 12, global round = 12), Loss/Aggregation: 1.560100893676281\n",
            "(epoch = 1, round = 12, global round = 12), Accuracy/Aggregation: 44.64\n",
            "Train finished Global Round: 13\n",
            "(epoch = 1, round = 13, global round = 13), Loss/Training: 1.4795783475041389\n",
            "(epoch = 1, round = 13, global round = 13), Accuracy/Training: 47.28\n",
            "reporting (epoch = 1, round = 13, global round = 13) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  65%|██████▌   | 13/20 [01:22<00:44,  6.30s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 13, global round = 13), Loss/Aggregation: 1.446392983198166\n",
            "(epoch = 1, round = 13, global round = 13), Accuracy/Aggregation: 48.88\n",
            "Train finished Global Round: 14\n",
            "(epoch = 1, round = 14, global round = 14), Loss/Training: 1.4036920696496964\n",
            "(epoch = 1, round = 14, global round = 14), Accuracy/Training: 49.84\n",
            "reporting (epoch = 1, round = 14, global round = 14) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  70%|███████   | 14/20 [01:28<00:37,  6.31s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 14, global round = 14), Loss/Aggregation: 1.4710052892565728\n",
            "(epoch = 1, round = 14, global round = 14), Accuracy/Aggregation: 47.44\n",
            "Train finished Global Round: 15\n",
            "(epoch = 1, round = 15, global round = 15), Loss/Training: 1.4139896921813488\n",
            "(epoch = 1, round = 15, global round = 15), Accuracy/Training: 49.88\n",
            "reporting (epoch = 1, round = 15, global round = 15) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  75%|███████▌  | 15/20 [01:34<00:31,  6.30s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 15, global round = 15), Loss/Aggregation: 1.4768752411007882\n",
            "(epoch = 1, round = 15, global round = 15), Accuracy/Aggregation: 48.16\n",
            "Train finished Global Round: 16\n",
            "(epoch = 1, round = 16, global round = 16), Loss/Training: 1.3692005164921284\n",
            "(epoch = 1, round = 16, global round = 16), Accuracy/Training: 50.32\n",
            "reporting (epoch = 1, round = 16, global round = 16) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  80%|████████  | 16/20 [01:41<00:25,  6.29s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 16, global round = 16), Loss/Aggregation: 1.4088042065501214\n",
            "(epoch = 1, round = 16, global round = 16), Accuracy/Aggregation: 48.04\n",
            "Train finished Global Round: 17\n",
            "(epoch = 1, round = 17, global round = 17), Loss/Training: 1.3533583544194698\n",
            "(epoch = 1, round = 17, global round = 17), Accuracy/Training: 50.88\n",
            "reporting (epoch = 1, round = 17, global round = 17) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  85%|████████▌ | 17/20 [01:47<00:18,  6.29s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 17, global round = 17), Loss/Aggregation: 1.4840220272541047\n",
            "(epoch = 1, round = 17, global round = 17), Accuracy/Aggregation: 46.44\n",
            "Train finished Global Round: 18\n",
            "(epoch = 1, round = 18, global round = 18), Loss/Training: 1.3278197191655636\n",
            "(epoch = 1, round = 18, global round = 18), Accuracy/Training: 51.16\n",
            "reporting (epoch = 1, round = 18, global round = 18) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  90%|█████████ | 18/20 [01:53<00:12,  6.29s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 18, global round = 18), Loss/Aggregation: 1.355546436458826\n",
            "(epoch = 1, round = 18, global round = 18), Accuracy/Aggregation: 51.08\n",
            "Train finished Global Round: 19\n",
            "(epoch = 1, round = 19, global round = 19), Loss/Training: 1.3134865760803223\n",
            "(epoch = 1, round = 19, global round = 19), Accuracy/Training: 52.48\n",
            "reporting (epoch = 1, round = 19, global round = 19) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  95%|█████████▌| 19/20 [01:59<00:06,  6.31s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 19, global round = 19), Loss/Aggregation: 1.3932992726564408\n",
            "(epoch = 1, round = 19, global round = 19), Accuracy/Aggregation: 48.8\n",
            "Train finished Global Round: 20\n",
            "(epoch = 1, round = 20, global round = 20), Loss/Training: 1.277140738070011\n",
            "(epoch = 1, round = 20, global round = 20), Accuracy/Training: 55.04\n",
            "reporting (epoch = 1, round = 20, global round = 20) for aggregation\n",
            "(epoch = 1, round = 20, global round = 20), Loss/Aggregation: 1.2535485073924064\n",
            "(epoch = 1, round = 20, global round = 20), Accuracy/Aggregation: 54.84\n",
            "Running (epoch = 1, round = 20, global round = 20) for Eval\n",
            "(epoch = 1, round = 20, global round = 20), Loss/Eval: 1.3051495634019374\n",
            "(epoch = 1, round = 20, global round = 20), Accuracy/Eval: 53.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Round:  95%|█████████▌| 19/20 [02:14<00:07,  7.10s/round]\n",
            "Epoch:   0%|          | 0/1 [02:14<?, ?epoch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "6af80324-793c-4678-9143-80c89f6b299b",
        "showInput": false,
        "customInput": null,
        "id": "EKc8SBAtO_wo"
      },
      "source": [
        "After training finishes, we evaluate the model and report the accuracy on the test set before finishing this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "6ebd30d1-c3eb-437b-8719-1056167baaf9",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "6ebd30d1-c3eb-437b-8719-1056167baaf9",
        "executionStopTime": 1636423399091,
        "executionStartTime": 1636423389997,
        "id": "PKUbnLbZO_wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc1003b-845b-484d-dc97-56df628f8553"
      },
      "source": [
        "# We can now test our trained model.\n",
        "trainer.test(\n",
        "    data_provider=data_provider,\n",
        "    metrics_reporter=MetricsReporter([Channel.STDOUT]),\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running (epoch = 1, round = 1, global round = 1) for Test\n",
            "(epoch = 1, round = 1, global round = 1), Loss/Test: 1.3051495634019374\n",
            "(epoch = 1, round = 1, global round = 1), Accuracy/Test: 53.22\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 53.22}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d8235e4e-b860-4554-aa5d-fa4a8c312467",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "oM2ozTxJO_wo"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we first showed how to get the data. We then built a data provider by sharding the data to simulate multiple client devices, each with their own data, and splitting each client's data into batches. \n",
        "We defined a simple CNN as our model, wrapped it with a model compatible with FL training, and moved it to GPU. \n",
        "Lastly, we set the hyperparameters for FL training, launched the training flow, and evaluated our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "462e5328-7112-48b5-abcc-b2599aeb9c94",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "uH-Gqo8UO_wo"
      },
      "source": [
        "### Additional resources\n",
        "\n",
        "- For a more in-depth understanding of this tutorial, check out [example_utils.py](https://github.com/facebookresearch/FLSim/blob/main/flsim/utils/example_utils.py) where we define the data loader, data provider, simple CNN, `FLModel`, and metrics reporter that we use in this tutorial.\n",
        "\n",
        "- [FLSim tutorials](https://github.com/facebookresearch/FLSim/tree/main/tutorials) - check out our other tutorial on sentiment classification.\n",
        "\n",
        "- Kairouz et al. (2021): [Advances and Open Problems in Federated Learning](https://arxiv.org/pdf/1912.04977.pdf). As the title suggests, an in-depth overview of advances and open problems in FL.\n",
        "\n",
        "- If you're interested in federated learning with differential privacy, take a look at [Opacus](https://opacus.ai/), a library that enables training PyTorch models with differential privacy. \n",
        "You can find a blog post introducing Opacus [here](https://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/).\n",
        "\n"
      ]
    }
  ]
}
