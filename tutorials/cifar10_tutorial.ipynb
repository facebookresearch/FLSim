{
  "metadata": {
    "kernelspec": {
      "display_name": "Python3",
      "language": "python",
      "name": "Python3",
      "metadata": null
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "last_server_session_id": "e12b224b-03c7-4a73-aabb-61388c7bba69",
    "last_kernel_id": "c0436f9e-4a57-4fb3-ac3e-9427e71e47af",
    "last_base_url": "",
    "last_msg_id": "48e6e693-558cd9719ac0f595fa22e69e_91",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "ec4510e7-5bb1-45a2-ac60-57ca1906cf03",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "# FLSim Tutorial: Image classification with CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "14df5562-53c7-42ee-bafa-a5bcc0e32826",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will train a simple CNN image classifier on CIFAR-10 with federated learning using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "a4acb953-0669-4c3f-8ce9-4d798c87ca7a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "To get the most of this tutorial, you should be comfortable training machine learning models with **PyTorch** and familiar with the concept of **federated learning (FL)**. If you are unfamimiliar with either of them or could use a refresher, please take a look at the following resources before proceeding with the tutorial:\n",
        "\n",
        "- McMahan & Ramage (2017): [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html). A short blog post from Google AI introducing the main idea of FL in a beginner-friendly way.\n",
        "- McMahan et al. (2017): [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/pdf/1602.05629.pdf). This paper first proposes the approach of federated learning. The described algorithm is now known as federated averaging (or FedAvg for short).\n",
        "- PyTorch has [extensive tutorials](https://pytorch.org/tutorials/) on their website. In particular, take a look at their [image classification tutorial using CIFAR-10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
        "\n",
        "Now that you're familiar with PyTorch and FL, let's move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "eeb9679b-564f-41f4-ac5e-68af58d18d0b",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### Objectives \n",
        "\n",
        "In this tutorial, you will learn how to \n",
        "\n",
        "1. Build a data pipeline for federated learning with FLSim,\n",
        "2. Create an image classification model compatible with FL training,\n",
        "3. Set hyperparameters for FL training, and\n",
        "4. Launch an FL training flow using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "b82a213b-1dc9-4d0b-be5f-24f7341f25bd",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "## Training an image classifier with FLSim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "e78c1595-1416-4a95-8ace-e281555b052a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### 0. About the dataset\n",
        "\n",
        "For this tutorial, we will use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR-10 dataset consists of 60k 3-channel color images with 32x32 pixels from 10 classes, with 6k images per class. \n",
        "There are 50k training images (5k training images per class) and 10k test images (1k test images per class).\n",
        "The classes are ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, and ‘truck’.\n",
        "\n",
        "![img](https://pytorch.org/tutorials/_images/cifar10.png)\n",
        "\n",
        "We can get the CIFAR-10 dataset from `torchvision.datasets`.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "f419c741-d171-4d8a-b731-4ae61916945c",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "da66e87d-279f-47ba-8a19-c1d1fc7423a4",
        "executionStartTime": 1636676239328,
        "executionStopTime": 1636676243207
      },
      "source": [
        "from torchvision.datasets.cifar import CIFAR10"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "f476eb9e-a006-4cbc-81e2-585e771e6707",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### 1. Data pipeline\n",
        "\n",
        "First, let's define how to build the data pipeline for federated learning. \n",
        "Recall that in FL, we have multiple client devices, each with their own data.\n",
        "This means that in addition to non-FL data processing, we need to split the dataset into multiple smaller dataset, which will each represent a client device's data.\n",
        "\n",
        "1. First, let us create data transforms and training, eval, and test datasets. This step is identical to preparing data in non-federated learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "80db107f-16aa-4869-bc88-272eea4f13dc",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "3c7434dd-be7f-437a-ab3b-42de32b9f2b3",
        "executionStartTime": 1636676243306,
        "executionStopTime": 1636676245430
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "\n",
        "# 1. Create training, eval, and test datasets like in non-federated learning.\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "train_dataset = CIFAR10(\n",
        "    root=\"./cifar10\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = CIFAR10(\n",
        "    root=\"./cifar10\", train=False, download=True, transform=transform\n",
        ")\n",
        ""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "0c8f6416-ee0b-42dd-b62a-772c3b0ab1ef",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "There are a few extra steps to enable training with federated learning. In particular, we need to\n",
        "\n",
        "2. Create a sharder, which defines a mapping from examples in the training data to clients. \n",
        "In other words, **a sharder groups rows of data into client datasets**, returning a list of list of examples. \n",
        "FLSim provides a number of sharding strategies such as random or column-based sharding. \n",
        "In this tutorial, we use sequential sharding, which assigns the first `SAMPLES_PER_CLIENT` rows to client 0, the second `SAMPLES_PER_CLIENT` rows to client 1, etc. \n",
        "\n",
        "3. Create a data loader, which will shard and batchify training, eval, and test data. \n",
        "For each dataset, the data loader first assigns rows to clients using the sharder and then splits each client's data into batches of size `batch_size`. \n",
        "We choose not to drop the last batch.\n",
        "\n",
        "4. Lastly, wrap the data loader with a data provider and return it. \n",
        "The data provider creates clients from the groupings in the data loader and adds metadata (e.g. number of examples, number of batches per client). \n",
        "Our data is now formatted such that the trainer will accept it.\n",
        "\n",
        "Note that the concept of a client or device only applies to the training data, the eval and test data are identical to non-federated learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "530a5c65-c2e0-4e88-a0b6-db863ee6764d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "5e93c86f-308a-40a1-ba3f-61d628adcde2",
        "executionStartTime": 1636676245525,
        "executionStopTime": 1636676309982
      },
      "source": [
        "from flsim.baselines.data_providers import FLVisionDataLoader, LEAFDataProvider\n",
        "from flsim.data.data_sharder import FLDataSharder, ShardingStrategyType\n",
        "\n",
        "\n",
        "SAMPLES_PER_CLIENT = 5000\n",
        "\n",
        "\n",
        "# 2. Create a sharder, which maps samples in the training data to clients.\n",
        "sharder = FLDataSharder(\n",
        "    ShardingStrategyType.SEQUENTIAL, shard_size_for_sequential=SAMPLES_PER_CLIENT\n",
        ")\n",
        "\n",
        "# 3. Shard and batchify training, eval, and test data.\n",
        "fl_data_loader = FLVisionDataLoader(\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    sharder=sharder,\n",
        "    batch_size=32,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# 4. Wrap the data loader with a data provider.\n",
        "data_provider = LEAFDataProvider(fl_data_loader)\n",
        "print(f\"Clients in total: {data_provider.num_users()}\")\n",
        ""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCreating FL User: 0user [00:00, ?user/s]",
            "\rCreating FL User: 1user [00:45, 45.73s/user]",
            "\rCreating FL User: 2user [00:45, 18.93s/user]",
            "\rCreating FL User: 3user [00:46, 10.37s/user]",
            "\rCreating FL User: 4user [00:46,  6.34s/user]",
            "\rCreating FL User: 5user [00:46,  4.12s/user]",
            "\rCreating FL User: 6user [00:46,  2.77s/user]",
            "\rCreating FL User: 7user [00:46,  1.92s/user]",
            "\rCreating FL User: 8user [00:46,  1.36s/user]",
            "\rCreating FL User: 9user [00:47,  1.01user/s]",
            "\rCreating FL User: 10user [00:47,  1.37user/s]",
            "\rCreating FL User: 10user [00:47,  4.73s/user]",
            "\n\rCreating FL User: 0user [00:00, ?user/s]",
            "\rCreating FL User: 1user [00:08,  8.58s/user]",
            "\rCreating FL User: 2user [00:08,  3.62s/user]",
            "\rCreating FL User: 2user [00:08,  4.37s/user]",
            "\n\rCreating FL User: 0user [00:00, ?user/s]",
            "\rCreating FL User: 1user [00:08,  8.29s/user]",
            "\rCreating FL User: 2user [00:08,  3.50s/user]",
            "\rCreating FL User: 2user [00:08,  4.22s/user]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clients in total: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "06d24749-da10-4a1b-a48d-3810fda4ebf9",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### 2. Create the model\n",
        "\n",
        "Now, let's see how we can create a model that is compatible with FL-training.\n",
        "\n",
        "1. First, we define a standard, non-FL image classification pytorch `nn.Module`; in this tutorial we use a simple CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7d1549e0-5da1-4480-b581-0c46747a301d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "d166aa7e-cb98-456c-812f-4da91045817e",
        "executionStartTime": 1636676310008,
        "executionStopTime": 1636676310017
      },
      "source": [
        "from flsim.baselines.models.cnn import SimpleConvNet\n",
        "\n",
        "# 1. Define our model, a simple CNN.\n",
        "model = SimpleConvNet(in_channels=3, num_classes=10)\n",
        "model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "SimpleConvNet(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n  (bn_relu): Sequential(\n    (0): GroupNorm(32, 32, eps=1e-05, affine=True)\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (dropout): Dropout(p=0, inplace=False)\n  (fc): Linear(in_features=288, out_features=10, bias=True)\n)"
          },
          "metadata": null,
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "21c6a5c0-87d3-4f44-b8d4-d56ccf5c5268",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "After we have our standard PyTorch model, we can\n",
        "\n",
        "2. Create a `torch.device` and choose where the model will be allocated (CUDA or CPU).\n",
        "\n",
        "3. Wrap the pytorch module with the FLSim `FLModel`. `FLModel` is accepted by the trainer and handles moving our model, data, and predictions to GPU if desired. It also collects and returns metrics for each batch it predicts on. You can find its implementation [here](https://github.com/facebookresearch/FLSim/blob/main/baselines/models/cv_model.py)\n",
        "\n",
        "4. Move the model to GPU and enable CUDA if desired.\n",
        "\n",
        "The model now supports FL training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "fa53c62c-7732-4f31-b73a-3fd61a9ac6e0",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "ea6de9c7-e19f-4a9f-a638-e4d01c276409",
        "executionStartTime": 1636676310024,
        "executionStopTime": 1636676310027
      },
      "source": [
        "import torch\n",
        "from flsim.baselines.models.cv_model import FLModel\n",
        "\n",
        "\n",
        "USE_CUDA = True\n",
        "\n",
        "# 2. Choose where the model will be allocated.\n",
        "cuda_enabled = torch.cuda.is_available() and USE_CUDA\n",
        "device = torch.device(f\"cuda:{0}\" if cuda_enabled else \"cpu\")\n",
        "\n",
        "# 3. Wrap the model in FLModel.\n",
        "global_model = FLModel(model, device)\n",
        "\n",
        "# 4. Enable CUDA if desired.\n",
        "if cuda_enabled:\n",
        "    global_model.fl_cuda()\n",
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "983d685b-8e16-4efa-8a10-ec4ab73a69bc",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### 3. Hyperparameters\n",
        "\n",
        "We can represent the hyperparameters for FL training in a JSON config. In particular, we specify a FedAvg implementation with 10 clients participating in each round.\n",
        "\n",
        "This config is passed to the FL trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "3cdafa88-21b4-4080-ba20-2890431b32dd",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "d2ae8b2f-60b8-4c90-932b-261a401872d4",
        "executionStartTime": 1636676310034,
        "executionStopTime": 1636676310041,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "json_config = {\n",
        "    \"trainer\": {\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        # there are different types of aggegator\n",
        "        # fed avg doesn't require lr, while others such as fed_sgd fed_adam do\n",
        "        \"aggregator\": {\"_base_\": \"base_fed_avg_sync_aggregator\"},\n",
        "        \"client\": {\n",
        "            # number of client's local epochs\n",
        "            \"epochs\": 1,\n",
        "            \"optimizer\": {\n",
        "                \"_base_\": \"base_optimizer_sgd\",\n",
        "                # client's local learning rate\n",
        "                \"lr\": 0.01,\n",
        "                # client's local momentum\n",
        "                \"momentum\": 0.9,\n",
        "            },\n",
        "        },\n",
        "        # type of user selection sampling\n",
        "        \"active_user_selector\": {\"_base_\": \"base_sequential_active_user_selector\"},\n",
        "        # number of users per round for aggregation\n",
        "        \"users_per_round\": 5,\n",
        "        # total number of global epochs\n",
        "        # total #rounds = ceil(total_users / users_per_round) * epochs\n",
        "        \"epochs\": 1,\n",
        "        # frequency of reporting train metrics\n",
        "        \"train_metrics_reported_per_epoch\": 10,\n",
        "        # frequency of evaluation per epoch\n",
        "        \"eval_epoch_frequency\": 1,\n",
        "        \"do_eval\": True,\n",
        "        # should we report train metrics after global aggregation\n",
        "        \"report_train_metrics_after_aggregation\": True,\n",
        "    }\n",
        "}\n",
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "7ded273a-0606-450c-9feb-2f04142b9ad8",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "Even though we recommend a JSON config for ease of representation, FLSim is compatible with the Hydra config system and can work with YAML configs just like any other [PyTorch Lightning](https://www.pytorchlightning.ai/) project. Here, we convert the JSON config to OmegaConf via Hydra for consumption by FLSim. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "6d3ffb23-268d-4f0d-851e-7ad7db77510d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "29b5746c-615f-4362-9353-8429015ab915",
        "executionStartTime": 1636676310079,
        "executionStopTime": 1636676344837
      },
      "source": [
        "import flsim.configs\n",
        "from flsim.utils.config_utils import fl_config_from_json\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "\n",
        "cfg = fl_config_from_json(json_config)\n",
        "print(OmegaConf.to_yaml(cfg))\n",
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainer:\n  _target_: flsim.trainers.sync_trainer.SyncTrainer\n  _recursive_: false\n  epochs: 1.0\n  do_eval: true\n  always_keep_trained_model: false\n  timeout_simulator:\n    _target_: ???\n    _recursive_: false\n  train_metrics_reported_per_epoch: 10\n  eval_epoch_frequency: 1.0\n  active_user_selector:\n    _target_: flsim.active_user_selectors.simple_user_selector.SequentialActiveUserSelector\n    _recursive_: false\n    user_selector_seed: null\n  report_train_metrics: true\n  report_train_metrics_after_aggregation: true\n  use_train_clients_for_aggregation_metrics: true\n  client:\n    _target_: flsim.clients.base_client.Client\n    _recursive_: false\n    epochs: 1\n    optimizer:\n      _target_: flsim.optimizers.local_optimizers.LocalOptimizerSGD\n      _recursive_: false\n      lr: 0.01\n      momentum: 0.9\n      weight_decay: 0.0\n    lr_scheduler:\n      _target_: ???\n      _recursive_: false\n      base_lr: 0.001\n    max_clip_norm_normalized: null\n    only_federated_params: true\n    random_seed: null\n    shuffle_batch_order: false\n    store_models_and_optimizers: false\n    track_multiple_selection: false\n  channel:\n    _target_: flsim.channels.base_channel.IdentityChannel\n    _recursive_: false\n  report_communication_metrics: false\n  aggregator:\n    _target_: flsim.optimizers.sync_aggregators.FedAvgSyncAggregator\n    _recursive_: false\n    reducer:\n      _target_: ???\n      _recursive_: false\n      only_federated_params: true\n    num_users_per_round: 1\n    total_number_of_users: 10000000000\n  users_per_round: 5\n  dropout_rate: 1.0\n  report_client_metrics_after_epoch: false\n  report_client_metrics: false\n  client_metrics_reported_per_epoch: 1\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d3296d38-38da-44ea-ad8e-305de0c3e06d",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### 4. Training\n",
        "Recall that we already built the data provider and created a model compatible with FL training. \n",
        "Now, to launch the FL training flow we only need to take a few more steps:\n",
        "\n",
        "1. First, we need to create a metric reporter, which will collect, evaluate, and report relevent training, aggretaion, and evaluation/test metrics.\n",
        "You can find its implementation [here](https://github.com/facebookresearch/FLSim/blob/main/tutorials/metrics_reporter/fl_metrics_reporter.py).\n",
        "\n",
        "2. We also need to instantiate the trainer with the model and hyperparameter config we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "eda9e453-6fd7-4489-be5c-d903070b0313",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "ea325384-6645-4710-af98-9e0716a26b82",
        "executionStartTime": 1636676344932,
        "executionStopTime": 1636676345079,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "from flsim.interfaces.metrics_reporter import Channel\n",
        "from flsim.tutorials.metrics_reporter.fl_metrics_reporter import MetricsReporter\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "\n",
        "# 1. Create a metric reporter.\n",
        "metrics_reporter = MetricsReporter([Channel.TENSORBOARD, Channel.STDOUT])\n",
        "\n",
        "# 2. Instantiate the trainer.\n",
        "trainer_config = cfg.trainer\n",
        "trainer = instantiate(trainer_config, model=global_model, cuda_enabled=cuda_enabled)\n",
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "7ed9a6f8-0889-40d6-9943-62a539cbbda8",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "Finally, we're ready to run FL training given the above JSON config. We can utilize `eval_score` to store the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "9ebd6625-78e2-4c61-a7e8-4216bf2facca",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "61319849-80cc-4bf0-9c82-ee39933ec1a7",
        "executionStartTime": 1636676345082,
        "executionStopTime": 1636676806656
      },
      "source": [
        "# Launch FL training.\n",
        "final_model, eval_score = trainer.train(\n",
        "    data_provider=data_provider,\n",
        "    metric_reporter=metrics_reporter,\n",
        "    num_total_users=data_provider.num_users(),\n",
        "    distributed_world_size=1,\n",
        ")\n",
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/1 [00:00<?, ?epoch/s]",
            "\rRound:   0%|          | 0/2 [00:00<?, ?round/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train finished Global Round: 1\n(epoch = 1, round = 1, global round = 1), Loss/Training: 1.929538930905093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 1, global round = 1), Accuracy/Training: 30.492\n(epoch = 1, round = 1, global round = 1), round_to_target/Training: 10000000000.0\nreporting (epoch = 1, round = 1, global round = 1) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 1, global round = 1), Loss/Aggregation: 1.620859497519815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  50%|█████     | 1/2 [02:51<02:51, 171.38s/round]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 1, global round = 1), Accuracy/Aggregation: 43.06\n(epoch = 1, round = 1, global round = 1), round_to_target/Aggregation: 10000000000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train finished Global Round: 2\n(epoch = 1, round = 2, global round = 2), Loss/Training: 1.6210210218551053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 2, global round = 2), Accuracy/Training: 41.252\n(epoch = 1, round = 2, global round = 2), round_to_target/Training: 10000000000.0\nreporting (epoch = 1, round = 2, global round = 2) for aggregation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 2, global round = 2), Loss/Aggregation: 1.3944371010847152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 2, global round = 2), Accuracy/Aggregation: 50.04\n(epoch = 1, round = 2, global round = 2), round_to_target/Aggregation: 10000000000.0\nRunning (epoch = 1, round = 2, global round = 2) for Eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 2, global round = 2), Loss/Eval: 1.408292086450917\n(epoch = 1, round = 2, global round = 2), Accuracy/Eval: 48.74\n(epoch = 1, round = 2, global round = 2), round_to_target/Eval: 10000000000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRound:  50%|█████     | 1/2 [07:41<07:41, 461.52s/round]",
            "\n\rEpoch:   0%|          | 0/1 [07:41<?, ?epoch/s]",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "6af80324-793c-4678-9143-80c89f6b299b",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "After training finishes, we evaluate the model and report the test set accuracy before concluding this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "6ebd30d1-c3eb-437b-8719-1056167baaf9",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "7d4def37-d67f-429f-b4fe-79afd77b654d",
        "executionStopTime": 1636676821408,
        "executionStartTime": 1636676806722
      },
      "source": [
        "# We can now test our model.\n",
        "trainer.test(\n",
        "    data_iter=data_provider.test_data(),\n",
        "    metric_reporter=MetricsReporter([Channel.STDOUT]),\n",
        ")\n",
        ""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running (epoch = 1, round = 1, global round = 1) for Test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch = 1, round = 1, global round = 1), Loss/Test: 1.408292086450917\n(epoch = 1, round = 1, global round = 1), Accuracy/Test: 48.74\n(epoch = 1, round = 1, global round = 1), round_to_target/Test: 10000000000.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'Accuracy': 48.74, 'round_to_target': 10000000000.0}"
          },
          "metadata": null,
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d8235e4e-b860-4554-aa5d-fa4a8c312467",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we first showed how to get the CIFAR-10 dataset. \n",
        "We then built a data provider by sharding the data to simulate multiple client devices, each with their own data, and splitting each client's data into batches. \n",
        "We defined a simple CNN as our model, wrapped it with a model compatible with FL training, and moved it to GPU. \n",
        "Lastly, we set the hyperparameters for FL training, launched the training flow, and evaluated our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "462e5328-7112-48b5-abcc-b2599aeb9c94",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "### Additional resources\n",
        "- [FLSim tutorials](https://github.com/facebookresearch/FLSim/tree/main/tutorials) - check out our other tutorial on sentiment classification.\n",
        "- Kairouz et al. (2021): [Advances and Open Problems in Federated Learning](https://arxiv.org/pdf/1912.04977.pdf). As the title suggests, an in-depth overview of advances and open problems in FL.\n",
        "- If you're interested in federated learning with **differential privacy**, take a look at [Opacus](https://opacus.ai/), a library that enables training PyTorch models with differential privacy. \n",
        "You can find a blog post introducing Opacus [here](https://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/).\n",
        "\n",
        ""
      ]
    }
  ]
}
