{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "last_server_session_id": "",
    "last_kernel_id": "",
    "last_base_url": "",
    "last_msg_id": "",
    "outputWidgetContext": {},
    "colab": {
      "name": "cifar10_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "ec4510e7-5bb1-45a2-ac60-57ca1906cf03",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "RcL6FFVmO_we"
      },
      "source": [
        "# FLSim Tutorial: Image classification with CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "14df5562-53c7-42ee-bafa-a5bcc0e32826",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "9gIQhqPSO_wg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will train a simple CNN image classifier on CIFAR-10 with federated learning using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "a4acb953-0669-4c3f-8ce9-4d798c87ca7a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "qCwSd63HO_wg"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "To get the most of this tutorial, you should be comfortable with training machine learning models with **PyTorch** and familiar with the concept of **federated learning (FL)**. If you are unfamimiliar with either of them or could use a refresher, please take a look at the following resources before proceeding with the tutorial:\n",
        "\n",
        "- McMahan & Ramage (2017): [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html). A short blog post from Google AI introducing the main idea of FL in a beginner-friendly way.\n",
        "- McMahan et al. (2017): [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/pdf/1602.05629.pdf). This paper first proposes the approach of federated learning. The described algorithm is now known as federated averaging (or FedAvg for short).\n",
        "- PyTorch has [extensive tutorials](https://pytorch.org/tutorials/) on their website. In particular, take a look at their [image classification tutorial using CIFAR-10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
        "\n",
        "Now that you're familiar with PyTorch and FL, let's move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "eeb9679b-564f-41f4-ac5e-68af58d18d0b",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "90vb3wIbO_wg"
      },
      "source": [
        "### Objectives \n",
        "\n",
        "By the end of this tutorial, we will have learnt how to\n",
        "\n",
        "1. Build a data pipeline for federated learning with FLSim,\n",
        "2. Create an image classification model compatible with FL training,\n",
        "3. Set hyperparameters for FL training, \n",
        "4. Create a metrics reporter to collect metrics, and\n",
        "5. Launch an FL training flow using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "b82a213b-1dc9-4d0b-be5f-24f7341f25bd",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "SEk7EMBXO_wh"
      },
      "source": [
        "## Training an image classifier with FLSim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-rYy6_w7kT"
      },
      "source": [
        "### Prerequisite\n",
        "First, let's install flsim via pip with the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdMaaGUWjWbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12b7df8-b589-4656-ddde-8655773fc7dc"
      },
      "source": [
        "!pip install --quiet flsim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 304 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 114 kB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.0 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9WDRb2y7rcM"
      },
      "source": [
        "USE_CUDA = True\n",
        "LOCAL_BATCH_SIZE = 32\n",
        "EXAMPLES_PER_USER = 500\n",
        "IMAGE_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "e78c1595-1416-4a95-8ace-e281555b052a",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "2DgQGgN0O_wi"
      },
      "source": [
        "### 0. About the dataset\n",
        "\n",
        "For this tutorial, we will use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR-10 dataset consists of 60k 3x32x32 3-channel color images with 32x32 pixels from 10 classes, with 6k images per class. \n",
        "There are 50k training images (5k training images per class) and 10k test images (1k test images per class).\n",
        "The classes are ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, and ‘truck’.\n",
        "\n",
        "We can get the CIFAR-10 dataset from `torchvision.datasets`.\n",
        "\n",
        "![img](https://pytorch.org/tutorials/_images/cifar10.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "f419c741-d171-4d8a-b731-4ae61916945c",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "f419c741-d171-4d8a-b731-4ae61916945c",
        "executionStartTime": 1636504268213,
        "executionStopTime": 1636504272232,
        "id": "ii-IOAM6O_wi"
      },
      "source": [
        "from torchvision.datasets.cifar import CIFAR10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "f476eb9e-a006-4cbc-81e2-585e771e6707",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "-NipUzoIO_wi"
      },
      "source": [
        "### 1. Data pipeline\n",
        "\n",
        "First, let's define how to build the data pipeline for federated learning:\n",
        "\n",
        "1. We create data transforms and training, eval, and test datasets. This step is identical to preparing data in non-federated learning.\n",
        "\n",
        "There are a few extra steps to enable training with federated learning. In particular, we need to\n",
        "\n",
        "2. Create a sharder, which defines a mapping from examples in the training data to clients. In other words, **a sharder groups rows of data into client datasets**, returning a list of list of examples. FLSim provides a number of sharding strategies such as random or column-based sharding. In this tutorial, we use sequential sharding, which assigns the first `examples_per_user` rows to user 0, the second `examples_per_user` rows to user 1, etc. \n",
        "\n",
        "3. Create a data loader, which will shard and batchify training, eval, and test data. For each dataset, the data loader first assigns rows to clients using the sharder and then splits each client's data into batches of size `batch_size`. We choose not to drop the last batch.\n",
        "\n",
        "4. Lastly, wrap the data loader with a data provider and return it. The data provider creates clients from the groupings in the data loader and adds metadata (e.g. number of examples/batches). Our data is now formatted such that the trainer will accept it.\n",
        "\n",
        "Note that the concept of a client or device only applies to the training data, the eval and test set data identical to non-federated learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q18yd-633lCh"
      },
      "source": [
        "import random\n",
        "from typing import Any, Dict, Generator, Iterable, Iterator, List, Tuple\n",
        "\n",
        "import torch\n",
        "from flsim.data.data_provider import IFLDataProvider, IFLUserData\n",
        "from flsim.data.data_sharder import FLDataSharder\n",
        "from flsim.interfaces.data_loader import IFLDataLoader\n",
        "from flsim.utils.data.data_utils import batchify\n",
        "from torchvision.datasets import VisionDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def collate_fn(batch: Tuple) -> Dict[str, Any]:\n",
        "    feature, label = batch\n",
        "    return {\"features\": feature, \"labels\": label}\n",
        "\n",
        "\n",
        "class DataLoader(IFLDataLoader):\n",
        "    SEED = 2137\n",
        "    random.seed(SEED)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_dataset: VisionDataset,\n",
        "        eval_dataset: VisionDataset,\n",
        "        test_dataset: VisionDataset,\n",
        "        sharder: FLDataSharder,\n",
        "        batch_size: int,\n",
        "        drop_last: bool = False,\n",
        "        collate_fn=collate_fn,\n",
        "    ):\n",
        "        assert batch_size > 0, \"Batch size should be a positive integer.\"\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        self.sharder = sharder\n",
        "        self.collate_fn = collate_fn\n",
        "\n",
        "    def fl_train_set(self, **kwargs) -> Iterable[Dict[str, Generator]]:\n",
        "        rank = kwargs.get(\"rank\", 0)\n",
        "        world_size = kwargs.get(\"world_size\", 1)\n",
        "        yield from self._batchify(self.train_dataset, self.drop_last, world_size, rank)\n",
        "\n",
        "    def fl_eval_set(self, **kwargs) -> Iterable[Dict[str, Generator]]:\n",
        "        yield from self._batchify(self.eval_dataset, drop_last=False)\n",
        "\n",
        "    def fl_test_set(self, **kwargs) -> Iterable[Dict[str, Generator]]:\n",
        "        yield from self._batchify(self.test_dataset, drop_last=False)\n",
        "\n",
        "    def _batchify(\n",
        "        self,\n",
        "        dataset: VisionDataset,\n",
        "        drop_last: bool = False,\n",
        "        world_size: int = 1,\n",
        "        rank: int = 0,\n",
        "    ) -> Generator[Dict[str, Generator], None, None]:\n",
        "        data_rows: List[Dict[str, Any]] = [self.collate_fn(batch) for batch in dataset]\n",
        "        for index, (_, user_data) in enumerate(self.sharder.shard_rows(data_rows)):\n",
        "            batch = {}\n",
        "            keys = user_data[0].keys()\n",
        "            for key in keys:\n",
        "                attribute = {\n",
        "                    key: batchify(\n",
        "                        [row[key] for row in user_data],\n",
        "                        self.batch_size,\n",
        "                        drop_last,\n",
        "                    )\n",
        "                }\n",
        "                batch = {**batch, **attribute}\n",
        "            yield batch\n",
        "\n",
        "\n",
        "class UserData(IFLUserData):\n",
        "    def __init__(self, user_data: Dict[str, Generator]):\n",
        "        self._user_batches = []\n",
        "        self._num_batches = 0\n",
        "        self._num_examples = 0\n",
        "        for features, labels in zip(user_data[\"features\"], user_data[\"labels\"]):\n",
        "            self._num_batches += 1\n",
        "            self._num_examples += UserData.get_num_examples(labels)\n",
        "            self._user_batches.append(UserData.fl_training_batch(features, labels))\n",
        "\n",
        "    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Iterator to return a user batch data\n",
        "        \"\"\"\n",
        "        for batch in self._user_batches:\n",
        "            yield batch\n",
        "\n",
        "    def num_examples(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the number of examples\n",
        "        \"\"\"\n",
        "        return self._num_examples\n",
        "\n",
        "    def num_batches(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the number of batches\n",
        "        \"\"\"\n",
        "        return self._num_batches\n",
        "\n",
        "    @staticmethod\n",
        "    def get_num_examples(batch: List) -> int:\n",
        "        return len(batch)\n",
        "\n",
        "    @staticmethod\n",
        "    def fl_training_batch(\n",
        "        features: List[torch.Tensor], labels: List[float]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        return {\"features\": torch.stack(features), \"labels\": torch.Tensor(labels)}\n",
        "\n",
        "\n",
        "class DataProvider(IFLDataProvider):\n",
        "    def __init__(self, data_loader):\n",
        "        self.data_loader = data_loader\n",
        "        self.train_users = self._create_fl_users(data_loader.fl_train_set())\n",
        "        self.eval_users = self._create_fl_users(data_loader.fl_eval_set())\n",
        "        self.test_users = self._create_fl_users(data_loader.fl_test_set())\n",
        "\n",
        "    def user_ids(self) -> List[int]:\n",
        "        return list(self.train_users.keys())\n",
        "\n",
        "    def num_users(self) -> int:\n",
        "        return len(self.train_users)\n",
        "\n",
        "    def get_user_data(self, user_index: int) -> IFLUserData:\n",
        "        if user_index in self.train_users:\n",
        "            return self.train_users[user_index]\n",
        "        else:\n",
        "            raise IndexError(\n",
        "                f\"Index {user_index} is out of bound for list with len {self.num_users()}\"\n",
        "            )\n",
        "\n",
        "    def train_data(self) -> Iterable[IFLUserData]:\n",
        "        for user_data in self.train_users.values():\n",
        "            yield user_data\n",
        "\n",
        "    def eval_data(self) -> Iterable[Dict[str, torch.Tensor]]:\n",
        "        for user_data in self.eval_users.values():\n",
        "            for batch in user_data:\n",
        "                yield batch\n",
        "\n",
        "    def test_data(self) -> Iterable[Dict[str, torch.Tensor]]:\n",
        "        for user_data in self.test_users.values():\n",
        "            for batch in user_data:\n",
        "                yield batch\n",
        "\n",
        "    def _create_fl_users(self, iterator: Iterator) -> Dict[int, IFLUserData]:\n",
        "        return {\n",
        "            user_index: UserData(user_data)\n",
        "            for user_index, user_data in tqdm(\n",
        "                enumerate(iterator), desc=\"Creating FL User\", unit=\"user\"\n",
        "            )\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "530a5c65-c2e0-4e88-a0b6-db863ee6764d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "7cf74fea-1b99-4995-b34e-f089c5f5c514",
        "executionStartTime": 1636523904177,
        "executionStopTime": 1636523904403,
        "id": "2eXJo2-CO_wj"
      },
      "source": [
        "from flsim.data.data_sharder import SequentialSharder\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def build_data_provider(local_batch_size, examples_per_user):\n",
        "\n",
        "    # 1. Create training, eval, and test datasets like in non-federated learning.\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(IMAGE_SIZE),\n",
        "            transforms.CenterCrop(IMAGE_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "    train_dataset = CIFAR10(\n",
        "        root=\"./cifar10\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = CIFAR10(\n",
        "        root=\"./cifar10\", train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    # 2. Create a sharder, which maps samples in the training data to clients.\n",
        "    sharder = SequentialSharder(examples_per_shard=examples_per_user)\n",
        "\n",
        "    # 3. Shard and batchify training, eval, and test data.\n",
        "    fl_data_loader = DataLoader(\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        test_dataset=test_dataset,\n",
        "        sharder=sharder,\n",
        "        batch_size=local_batch_size,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # 4. Wrap the data loader with a data provider.\n",
        "    data_provider = DataProvider(fl_data_loader)\n",
        "    print(f\"Clients in total: {data_provider.num_users()}\")\n",
        "    return data_provider\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "06d24749-da10-4a1b-a48d-3810fda4ebf9",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "tpZ2H7jNO_wk"
      },
      "source": [
        "### 2. Create the model\n",
        "\n",
        "Now, let's see how we can create a model that is compatible with FL-training.\n",
        "\n",
        "1. Define a standard, non-FL image classification pytorch `nn.Module`; in this tutorial we use a simple CNN.\n",
        "\n",
        "2. Create a `torch.device` and choose where the model will be allocated (CUDA or CPU).\n",
        "\n",
        "3. Wrap the pytorch module with the FLSim `FLModel`, an abstracted version of a FL friendly model class.\n",
        "\n",
        "4. Move the model to GPU and enable CUDA if desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJvSA68tCk_d"
      },
      "source": [
        "In this step, we create a standard nn.Module with 4 convolution layers, group norm and a linear layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7d1549e0-5da1-4480-b581-0c46747a301d",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "7d1549e0-5da1-4480-b581-0c46747a301d",
        "executionStartTime": 1636422569684,
        "executionStopTime": 1636422570122,
        "id": "ktgmrbTyO_wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdb7cce-685b-4899-b601-fdcf43557563"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, dropout_rate=0):\n",
        "        super(SimpleConvNet, self).__init__()\n",
        "        self.out_channels = 32\n",
        "        self.stride = 1\n",
        "        self.padding = 2\n",
        "        self.layers = []\n",
        "        in_dim = in_channels\n",
        "        for _ in range(4):\n",
        "          self.layers.append(nn.Conv2d(in_dim, self.out_channels, 3, self.stride, self.padding))\n",
        "          in_dim = self.out_channels\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "\n",
        "        self.gn_relu = nn.Sequential(\n",
        "            nn.GroupNorm(self.out_channels, self.out_channels, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        num_features = (\n",
        "            self.out_channels\n",
        "            * (self.stride + self.padding)\n",
        "            * (self.stride + self.padding)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.layers:\n",
        "            x = self.gn_relu(conv(x))\n",
        "\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = self.fc(self.dropout(x))\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "# 1. Define our model, a simple CNN.\n",
        "model = SimpleConvNet(in_channels=3, num_classes=10)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleConvNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  )\n",
              "  (gn_relu): Sequential(\n",
              "    (0): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0, inplace=False)\n",
              "  (fc): Linear(in_features=288, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tka3JhCyDx"
      },
      "source": [
        "Below, we wrap the pytorch module within an IFLModel. An IFLModel is an abstracted version of a FL friendly model class. It handles metrics, local training batch creation, training forward and eval forward. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "fa53c62c-7732-4f31-b73a-3fd61a9ac6e0",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "fa53c62c-7732-4f31-b73a-3fd61a9ac6e0",
        "executionStartTime": 1636422570217,
        "executionStopTime": 1636422570544,
        "id": "i1wVfw3OO_wl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "0a7033ea-f105-4cea-e4a5-c59499375350"
      },
      "source": [
        "import torch\n",
        "from typing import Optional\n",
        "import torch.nn.functional as F\n",
        "from flsim.interfaces.model import IFLModel\n",
        "from flsim.utils.simple_batch_metrics import FLBatchMetrics\n",
        "\n",
        "\n",
        "class FLModel(IFLModel):\n",
        "    def __init__(self, model: nn.Module, device: Optional[str] = None):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def fl_forward(self, batch) -> FLBatchMetrics:\n",
        "        features = batch[\"features\"]  # [B, C, 28, 28]\n",
        "        batch_label = batch[\"labels\"]\n",
        "        stacked_label = batch_label.view(-1).long().clone().detach()\n",
        "        if self.device is not None:\n",
        "            features = features.to(self.device)\n",
        "\n",
        "        output = self.model(features)\n",
        "\n",
        "        if self.device is not None:\n",
        "            output, batch_label, stacked_label = (\n",
        "                output.to(self.device),\n",
        "                batch_label.to(self.device),\n",
        "                stacked_label.to(self.device),\n",
        "            )\n",
        "\n",
        "        loss = F.cross_entropy(output, stacked_label)\n",
        "        num_examples = self.get_num_examples(batch)\n",
        "        output = output.detach().cpu()\n",
        "        stacked_label = stacked_label.detach().cpu()\n",
        "        del features\n",
        "        return FLBatchMetrics(\n",
        "            loss=loss,\n",
        "            num_examples=num_examples,\n",
        "            predictions=output,\n",
        "            targets=stacked_label,\n",
        "            model_inputs=[],\n",
        "        )\n",
        "\n",
        "    def fl_create_training_batch(self, **kwargs):\n",
        "        features = kwargs.get(\"features\", None)\n",
        "        labels = kwargs.get(\"labels\", None)\n",
        "        return UserData.fl_training_batch(features, labels)\n",
        "\n",
        "    def fl_get_module(self) -> nn.Module:\n",
        "        return self.model\n",
        "\n",
        "    def fl_cuda(self) -> None:\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_eval_metrics(self, batch) -> FLBatchMetrics:\n",
        "        with torch.no_grad():\n",
        "            return self.fl_forward(batch)\n",
        "\n",
        "    def get_num_examples(self, batch) -> int:\n",
        "        return UserData.get_num_examples(batch[\"labels\"])\n",
        "\n",
        "\n",
        "# 2. Choose where the model will be allocated.\n",
        "cuda_enabled = torch.cuda.is_available() and USE_CUDA\n",
        "device = torch.device(f\"cuda:{0}\" if cuda_enabled else \"cpu\")\n",
        "\n",
        "# 3. Wrap the model in FLModel.\n",
        "global_model = FLModel(model, device)\n",
        "\n",
        "# 4. Enable CUDA if desired.\n",
        "if cuda_enabled:\n",
        "    global_model.fl_cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-28989d9eea28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIFLModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_batch_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFLBatchMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flsim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJETpvZtDP0O"
      },
      "source": [
        "### 3. Metrics Reporting\n",
        "\n",
        "After we created our data pipeline and FL model, we then define our metrics reporter. The metrics reporter allows us to collect metrics and log them onto tensorboard. \n",
        "\n",
        "There are three functions that we care about: \n",
        "\n",
        "1. `compare_metrics`: This function compares the current eval metric that is returned from `create_eval_metrics` which we will define below. \n",
        "\n",
        "2. `compute_scores`: This function calculates the metrics that we care both. In this case, we would like to report the top1 accuracy. \n",
        "\n",
        "3. `create_eval_metrics`: This function creates the eval metrics dictionary that can be used by `compare_metrics` above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3dLLIn7ExUq"
      },
      "source": [
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from flsim.common.timeline import Timeline\n",
        "from flsim.interfaces.metrics_reporter import Channel, TrainingStage\n",
        "from flsim.metrics_reporter.tensorboard_metrics_reporter import FLMetricsReporter\n",
        "from flsim.utils.fl.stats import (\n",
        "    AverageType,\n",
        ")\n",
        "\n",
        "class MetricsReporter(FLMetricsReporter):\n",
        "    ACCURACY = \"Accuracy\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: List[Channel],\n",
        "        target_eval: float = 0.0,\n",
        "        window_size: int = 5,\n",
        "        average_type: str = \"sma\",\n",
        "        log_dir: Optional[str] = None,\n",
        "    ):\n",
        "        super().__init__(channels, log_dir)\n",
        "        self.set_summary_writer(log_dir=log_dir)\n",
        "        self._round_to_target = float(1e10)\n",
        "\n",
        "    def compare_metrics(self, eval_metrics, best_metrics):\n",
        "        print(f\"Current eval accuracy: {eval_metrics}%, Best so far: {best_metrics}%\")\n",
        "        if best_metrics is None:\n",
        "            return True\n",
        "\n",
        "        current_accuracy = eval_metrics.get(self.ACCURACY, float(\"-inf\"))\n",
        "        best_accuracy = best_metrics.get(self.ACCURACY, float(\"-inf\"))\n",
        "        return current_accuracy > best_accuracy\n",
        "\n",
        "    def compute_scores(self) -> Dict[str, Any]:\n",
        "        # compute accuracy\n",
        "        correct = torch.Tensor([0])\n",
        "        for i in range(len(self.predictions_list)):\n",
        "            all_preds = self.predictions_list[i]\n",
        "            pred = all_preds.data.max(1, keepdim=True)[1]\n",
        "\n",
        "            assert pred.device == self.targets_list[i].device, (\n",
        "                f\"Pred and targets moved to different devices: \"\n",
        "                f\"pred >> {pred.device} vs. targets >> {self.targets_list[i].device}\"\n",
        "            )\n",
        "            if i == 0:\n",
        "                correct = correct.to(pred.device)\n",
        "\n",
        "            correct += pred.eq(self.targets_list[i].data.view_as(pred)).sum()\n",
        "\n",
        "        # total number of data\n",
        "        total = sum(len(batch_targets) for batch_targets in self.targets_list)\n",
        "\n",
        "        accuracy = 100.0 * correct.item() / total\n",
        "        return {self.ACCURACY: accuracy}\n",
        "\n",
        "    def create_eval_metrics(\n",
        "        self, scores: Dict[str, Any], total_loss: float, **kwargs\n",
        "    ) -> Any:\n",
        "        timeline: Timeline = kwargs.get(\"timeline\", Timeline(global_round=1))\n",
        "        stage: TrainingStage = kwargs.get(\"stage\", None)\n",
        "        accuracy = scores[self.ACCURACY]\n",
        "        return {\n",
        "            self.ACCURACY: accuracy\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "983d685b-8e16-4efa-8a10-ec4ab73a69bc",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "ZmvB2-aNO_wl"
      },
      "source": [
        "### 4. Hyperparameters\n",
        "\n",
        "We can represent the hyperparameters for FL training in a JSON config. In particular, we specify a FedAvg implementation with 10 users per round.\n",
        "\n",
        "This config is passed to the FL trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "3cdafa88-21b4-4080-ba20-2890431b32dd",
        "showInput": true,
        "customInput": null,
        "requestMsgId": "3cdafa88-21b4-4080-ba20-2890431b32dd",
        "executionStartTime": 1636422570651,
        "executionStopTime": 1636422571503,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "nMCgbRFEO_wl"
      },
      "source": [
        "import flsim.configs\n",
        "from flsim.utils.config_utils import fl_config_from_json\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "\n",
        "json_config = {\n",
        "    \"trainer\": {\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        # there are different types of aggegator\n",
        "        # fed avg doesn't require lr, while others such as fed_avg_with_lr or fed_adam do\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        \"server\": {\n",
        "          \"_base_\": \"base_sync_server\",\n",
        "          \"server_optimizer\": {\n",
        "            \"_base_\": \"base_fed_avg\",\n",
        "          },\n",
        "          # type of user selection sampling\n",
        "          \"active_user_selector\": {\"_base_\": \"base_uniformly_random_active_user_selector\"},\n",
        "        },\n",
        "        \"client\": {\n",
        "            # number of client's local epoch\n",
        "            \"epochs\": 1,\n",
        "            \"optimizer\": {\n",
        "                \"_base_\": \"base_optimizer_sgd\",\n",
        "                # client's local learning rate\n",
        "                \"lr\": 1,\n",
        "                # client's local momentum\n",
        "                \"momentum\": 0.0,\n",
        "            },\n",
        "        },\n",
        "        # number of users per round for aggregation\n",
        "        \"users_per_round\": 5,\n",
        "        # total number of global epochs\n",
        "        # total #rounds = ceil(total_users / users_per_round) * epochs\n",
        "        \"epochs\": 1,\n",
        "        # frequentcy of reporting train metrics\n",
        "        \"train_metrics_reported_per_epoch\": 100,\n",
        "        # frequency of evaluation per epoch\n",
        "        \"eval_epoch_frequency\": 1,\n",
        "        \"do_eval\": True,\n",
        "        # should we report train metrics after global aggregation\n",
        "        \"report_train_metrics_after_aggregation\": True,\n",
        "    }\n",
        "}\n",
        "cfg = fl_config_from_json(json_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d3296d38-38da-44ea-ad8e-305de0c3e06d",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "GBvhMLL1O_wm"
      },
      "source": [
        "### 5. Training\n",
        "\n",
        "Finally, putting all the above together, to launch the FL training flow we\n",
        "\n",
        "1. Build the data provider,\n",
        "2. Create an FL model,\n",
        "3. Create a metric reporter,\n",
        "4. Instantiate the trainer,\n",
        "5. Launch training,\n",
        "6. Test the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "432ed49e-f3a8-469e-b13c-e7a484b07c3e",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "432ed49e-f3a8-469e-b13c-e7a484b07c3e",
        "executionStartTime": 1636422571609,
        "executionStopTime": 1636422644046,
        "id": "iHIREkSXO_wm"
      },
      "source": [
        "# 1. Build the data provider.\n",
        "data_provider = build_data_provider(\n",
        "    local_batch_size=LOCAL_BATCH_SIZE,\n",
        "    examples_per_user=EXAMPLES_PER_USER\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "1d7d4e7d-210f-4fba-adf7-3c61d00b6f17",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "1d7d4e7d-210f-4fba-adf7-3c61d00b6f17",
        "executionStopTime": 1636422644377,
        "executionStartTime": 1636422644062,
        "id": "uBSTkq15O_wm"
      },
      "source": [
        "# 2. We already defined the FL model earlier.\n",
        "global_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cswsStQJaoX"
      },
      "source": [
        "We can get the `nn.Module` by doing this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeD_oikAJYUO"
      },
      "source": [
        "global_model.fl_get_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "eda9e453-6fd7-4489-be5c-d903070b0313",
        "showInput": true,
        "customInput": null,
        "requestMsgId": "eda9e453-6fd7-4489-be5c-d903070b0313",
        "executionStartTime": 1636422644482,
        "executionStopTime": 1636422644774,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "eJ1EID9xO_wn"
      },
      "source": [
        "from flsim.interfaces.metrics_reporter import Channel\n",
        "\n",
        "# 3. Create a metric reporter.\n",
        "metrics_reporter = MetricsReporter([Channel.TENSORBOARD, Channel.STDOUT])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2eccb04f-229a-43f9-8310-0ced64132e2b",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "2eccb04f-229a-43f9-8310-0ced64132e2b",
        "executionStartTime": 1636422644878,
        "executionStopTime": 1636422645173,
        "id": "kLWIpCxjO_wn"
      },
      "source": [
        "from hydra.utils import instantiate\n",
        "\n",
        "# 4. Instantiate the trainer.\n",
        "trainer_config = cfg.trainer\n",
        "trainer = instantiate(trainer_config, model=global_model, cuda_enabled=cuda_enabled)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "7ed9a6f8-0889-40d6-9943-62a539cbbda8",
        "showInput": false,
        "customInput": null,
        "id": "ubqyiFg7O_wn"
      },
      "source": [
        "We run FL training given the above JSON config and utilize `eval_score` to store the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "9ebd6625-78e2-4c61-a7e8-4216bf2facca",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "9ebd6625-78e2-4c61-a7e8-4216bf2facca",
        "executionStartTime": 1636422645182,
        "executionStopTime": 1636423389881,
        "id": "JrF1nkjAO_wn"
      },
      "source": [
        "# 5. Launch FL training.\n",
        "final_model, eval_score = trainer.train(\n",
        "    data_provider=data_provider,\n",
        "    metric_reporter=metrics_reporter,\n",
        "    num_total_users=data_provider.num_users(),\n",
        "    distributed_world_size=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "6af80324-793c-4678-9143-80c89f6b299b",
        "showInput": false,
        "customInput": null,
        "id": "EKc8SBAtO_wo"
      },
      "source": [
        "After training finishes, we evaluate the model and report the test set accuracy before finishing this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "6ebd30d1-c3eb-437b-8719-1056167baaf9",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "requestMsgId": "6ebd30d1-c3eb-437b-8719-1056167baaf9",
        "executionStopTime": 1636423399091,
        "executionStartTime": 1636423389997,
        "id": "PKUbnLbZO_wo"
      },
      "source": [
        "# 6. We can now test our model.\n",
        "trainer.test(\n",
        "    data_iter=data_provider.test_data(),\n",
        "    metric_reporter=MetricsReporter([Channel.STDOUT]),\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d8235e4e-b860-4554-aa5d-fa4a8c312467",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "oM2ozTxJO_wo"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we first showed how to get the data. We then built a data provider by sharding the data to simulate multiple client devices, each with their own data, and split each client's data into batches. For our model, we defined a simple CNN, wrapped it with a model compatible with FL training, and moved it to GPU. Lastly, we defined hyperparameters and kicked off training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "462e5328-7112-48b5-abcc-b2599aeb9c94",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "id": "uH-Gqo8UO_wo"
      },
      "source": [
        "### Additional resources\n",
        "\n",
        "- Kairouz et al. (2021): [Advances and Open Problems in Federated Learning](https://arxiv.org/pdf/1912.04977.pdf). As the title suggests, an in-depth overview of advances and open problems in FL.\n",
        "\n"
      ]
    }
  ]
}
