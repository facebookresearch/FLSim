{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "1dwlL58esQ7x",
        "originalKey": "ef12966a-7e07-446b-b931-4235e269f994",
        "showInput": false
      },
      "source": [
        "# FLSim Tutorial: Sentiment Classification with LEAF's Sent140\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "OP4Clr_PsQ70",
        "originalKey": "669ad0e3-282b-41be-aad0-2c111b3402be",
        "showInput": false
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will train a binary sentiment classifier on LEAF's Sent140 dataset with federated learning using FLSim. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "yRlTlsNAsQ71",
        "originalKey": "bcb5f56d-908e-4978-9e60-73c794d1de79",
        "showInput": false
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "To get the most of this tutorial, you should be comfortable with training machine learning models with **PyTorch** and familiar with the concept of **federated learning (FL)**. If you are unfamiliar with either of them or could use a refresher, please take a look at the following resources before proceeding with the tutorial:\n",
        "\n",
        "- McMahan & Ramage (2017): [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html). A short blog post from Google AI introducing the main idea of FL in a beginner-friendly way.\n",
        "- McMahan et al. (2017): [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/pdf/1602.05629.pdf). This paper first proposes the approach of federated learning. The described algorithm is now known as federated averaging (or FedAvg for short).\n",
        "- PyTorch has [extensive tutorials](https://pytorch.org/tutorials/) on their website.\n",
        "- If you're new to **sentiment classification**, you can find Pang and Lee's survey on the topic [here](https://www.cs.cornell.edu/home/llee/omsa/omsa-published.pdf). \n",
        "\n",
        "Now that you're familiar with PyTorch and FL and have a sense of sentiment classification, let's move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "uu57DhzisQ71",
        "originalKey": "6c37e934-b30b-4946-8acf-f308b487688f",
        "showInput": false
      },
      "source": [
        "### Objectives \n",
        "\n",
        "By the end of this tutorial, we will have learnt how to\n",
        "\n",
        "1. Build a data pipeline for federated learning with FLSim,\n",
        "2. Create a sentiment classification model compatible with FL training,\n",
        "3. Create a metrics reporter to collect and report metrics,\n",
        "4. Set hyperparameters for FL training, and\n",
        "5. Launch an FL training flow using FLSim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "N9gqo4t2sQ72",
        "originalKey": "b2a57ec9-bb33-4df6-9608-0abe10445108",
        "showInput": false
      },
      "source": [
        "## Training a sentiment classifier with FLSim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OwUiKzUaW0o"
      },
      "source": [
        "### Prerequisites\n",
        "First, let us install flsim via pip with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGWFqWOgaWlC",
        "outputId": "aabca425-cc1c-4f72-ee1e-a1a150192d7a"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet flsim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKrpO_9QUTNi"
      },
      "source": [
        "Some useful parameters for later - no need to change these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1CLgWoXk2Vqy"
      },
      "outputs": [],
      "source": [
        "USE_CUDA = True\n",
        "LOCAL_BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 60\n",
        "\n",
        "# suppress large outputs\n",
        "VERBOSE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "4-obozTKsQ72",
        "originalKey": "5d2a8203-16cb-492a-a4cd-46b0a9456e21",
        "showInput": false
      },
      "source": [
        "### 0. About the dataset\n",
        "\n",
        "For this tutorial, we will use [LEAF's](https://leaf.cmu.edu/) [Sentiment140 (Sent140) dataset](https://leaf.cmu.edu/build/html/tutorials/sent140-md.html), which consists of 1.6 million tweets by 660k users. Note that the mean number of tweets per user is 2.42 and the standard deviation is 4.71.\n",
        "\n",
        "![Sent140 distribution of samples across users](https://leaf.cmu.edu/webpage/images/twitter_hist.png)\n",
        "\n",
        "Before the next step in this tutorial, we need to download the dataset and partition the data by users. \n",
        "In particular, we sample \n",
        "1% of the entire dataset (`--sf 0.01`) \n",
        "in a non-IID manner (`-s niid`) \n",
        "and partition 90% of sampled users into train and 10% of sampled users \n",
        "into test (`--tf 0.90`) as opposed to splitting individual samples into train and test (`-t 'user'`) .\n",
        "We require all users to have at least one sample (`-k 1`).\n",
        "\n",
        "For more information on the various preprocessing options, see [here](https://github.com/TalwalkarLab/leaf/tree/master/data/sent140). You can find the LEAF paper [here](https://arxiv.org/pdf/1812.01097.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "executionStartTime": 1636676289207,
        "executionStopTime": 1636676290023,
        "hidden_ranges": [],
        "id": "GIk4gCUqsQ73",
        "originalKey": "59d5e611-102e-4d44-9478-cfd54f0ca81b",
        "requestMsgId": "5ab54fa9-8358-41b9-91c5-ed36729a10b2",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "%%capture preprocess_output\n",
        "\n",
        "# Download and preprocess the data\n",
        "!git clone https://github.com/TalwalkarLab/leaf.git\n",
        "%cd leaf/data/shakespeare\n",
        "!./preprocess.sh -s niid --sf 0.2 -k 0 -t sample -tf 0.8 --raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2hWeBZQuftab"
      },
      "outputs": [],
      "source": [
        "if VERBOSE: print(preprocess_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "qtUAyO01sQ75",
        "originalKey": "6d9d0efd-fcaa-4017-beb6-c9da31b4094e",
        "showInput": false
      },
      "source": [
        "Let us now find our preprocessed data. Note that if you use different preprocessing options, you might need to change these!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALL_DATA = !ls data/all_data\n",
        "ALL_DATA = \"data/all_data/\" + ALL_DATA[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676290310,
        "executionStopTime": 1636676290315,
        "hidden_ranges": [],
        "id": "LZr-3AiXsQ76",
        "originalKey": "a27a936f-9c09-4884-a792-0cd118687116",
        "outputId": "a9a73a86-1f7d-43d8-cc68-c32303d7da6d",
        "requestMsgId": "4fb9c5e3-9a89-4081-b589-f9e1286bce9e",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['all_data_niid_2_keep_0_train_9.json']\n",
            "data/train/all_data_niid_2_keep_0_train_9.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('data/train/all_data_niid_2_keep_0_train_9.json',\n",
              " 'data/test/all_data_niid_2_keep_0_test_9.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TRAIN_DATA = !ls data/train\n",
        "print(TRAIN_DATA)\n",
        "TRAIN_DATA = \"data/train/\" + TRAIN_DATA[0]\n",
        "\n",
        "TEST_DATA = !ls data/test\n",
        "print(TRAIN_DATA)\n",
        "TEST_DATA = \"data/test/\" + TEST_DATA[0]\n",
        "\n",
        "TRAIN_DATA, TEST_DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "I19fSlqTsQ77",
        "originalKey": "20691777-83ab-4406-84c5-950fa4b2224f",
        "showInput": false
      },
      "source": [
        "We can now load the training data and get a sense of how samples are distributed across users in our subset of the Sent140 dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionStartTime": 1636676290335,
        "executionStopTime": 1636676290360,
        "hidden_ranges": [],
        "id": "rls5a-z9sQ77",
        "originalKey": "8cc146cc-61db-445c-9c8c-8528226d3b80",
        "outputId": "b05097e8-9c55-4fff-fb65-98678039b665",
        "requestMsgId": "26f25906-c0a5-43fe-a025-e4b39a598570",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of samples per user:\n",
            "  min=3, \n",
            "  max=66903, \n",
            "  median=1163.0, \n",
            "  mean=3743.28, \n",
            "  std=6212.26\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# load the training data\n",
        "with open(ALL_DATA, \"r\") as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "# how samples are distributed across users\n",
        "n_samples = training_data['num_samples']\n",
        "print(f\"\"\"\\nNumber of samples per user:\n",
        "  min={np.min(n_samples)}, \n",
        "  max={np.max(n_samples)}, \n",
        "  median={np.median(n_samples)}, \n",
        "  mean={np.mean(n_samples):.2f}, \n",
        "  std={np.std(n_samples):.2f}\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQjXxWha_AX"
      },
      "source": [
        "Let us also look at the data for an example user. Notice that there are multiple metadata fields in addition to the tweet itself and its sentiment label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676290400,
        "executionStopTime": 1636676290432,
        "hidden_ranges": [],
        "id": "8_zPhV2DsQ78",
        "originalKey": "4d8c298b-b1c4-48e9-bf09-0217abe97393",
        "outputId": "24ab5db4-6cb7-4d4a-8a79-7e348de64030",
        "requestMsgId": "a706da0d-e298-4e63-b2c0-c5ac97a43f64",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go, ye giddy goose.\n",
            "                                            The music plays.\n",
            "Then should you be nothing but musical; for you are\n",
            "altogether govern'd by humours. Lie still, ye thief, and hear the\n",
            "lady sing in Welsh.\n",
            "Wouldst thou have thy head broken?\n",
            "Then be still.\n",
            "Now God help thee!\n",
            "What's that?\n",
            "Not mine, in good sooth.\n",
            "I will not sing.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EXAMPLE_USER = training_data[\"users\"][56]\n",
        "print(training_data[\"user_data\"][EXAMPLE_USER][\"raw\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "VVyMleGnsQ79",
        "originalKey": "7152ba71-9205-408c-a369-00308e75314a",
        "showInput": false
      },
      "source": [
        "### 1. Data pipeline\n",
        "\n",
        "Now, let us define how to build the data pipeline for federated learning:\n",
        "\n",
        "1. To load the training and test data, we define a new dataset class, `ShakespeareDataset`, which converts each user verses into a `torch.Tensor`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "code_folding": [],
        "executionStartTime": 1636676290438,
        "executionStopTime": 1636676291421,
        "hidden_ranges": [],
        "id": "Y6hlQ2JKsQ79",
        "originalKey": "77651659-ce0a-48ef-947b-ca898bb3793f",
        "requestMsgId": "791e752f-82c1-444d-9ced-3716af1460c9"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data_root, max_seq_len):\n",
        "        self.data_root = data_root\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.char2idx = {c: i for i, c in enumerate(string.printable)}\n",
        "        self.idx2char = {i: u for i, u in enumerate(string.printable)}\n",
        "        self.num_letters = len(self.char2idx)\n",
        "        self.UNK = self.num_letters\n",
        "\n",
        "        with open(data_root, \"r+\") as f:\n",
        "            self.dataset = json.load(f)\n",
        "\n",
        "        self.data = {}\n",
        "        self.targets = {}\n",
        "\n",
        "        # Populate self.data and self.targets\n",
        "        for user_id, user_data in self.dataset[\"user_data\"].items():\n",
        "            self.data[user_id] = self.process_x(list(user_data[\"x\"]))\n",
        "            self.targets[user_id] = self.process_y(list(user_data[\"y\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for user_id in self.data.keys():\n",
        "            yield self.__getitem__(user_id)\n",
        "\n",
        "    def __getitem__(self, user_id: str):\n",
        "        if user_id not in self.data or user_id not in self.targets:\n",
        "            raise IndexError(f\"User {user_id} is not in dataset\")\n",
        "        return self.data[user_id], self.targets[user_id]\n",
        "\n",
        "    def line_to_indices(self, line: str, max_seq_len: int):\n",
        "        line_list = self.split_line(line)  # split phrase in words\n",
        "        line_list = line_list\n",
        "        chars = self.flatten_list([list(word) for word in line_list])\n",
        "        indices = [self.char2idx[letter]\n",
        "                   if letter in self.char2idx else self.UNK \n",
        "                   for i, letter in enumerate(chars) if i < max_seq_len\n",
        "          ]\n",
        "        # Add padding\n",
        "        indices = indices + [self.UNK] * (max_seq_len - len(indices))\n",
        "        return indices\n",
        "\n",
        "    def process_x(self, raw_x_batch):\n",
        "        x_batch = [self.line_to_indices(e, self.max_seq_len) for e in raw_x_batch]\n",
        "        x_batch = torch.LongTensor(x_batch)\n",
        "        return x_batch\n",
        "\n",
        "    def process_y(self, raw_y_batch):\n",
        "        y_batch = [self.char2idx[e] for e in raw_y_batch]\n",
        "        return y_batch\n",
        "\n",
        "    def split_line(self, line):\n",
        "        \"\"\"\n",
        "        Split given line/phrase (str) into list of words (List[str])\n",
        "        \"\"\"\n",
        "        return re.findall(r\"[\\w']+|[.,!?;]\", line)\n",
        "\n",
        "    def flatten_list(self, nested_list):\n",
        "        return list(itertools.chain.from_iterable(nested_list))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "SoqA5GkSsQ79",
        "originalKey": "f76ded33-fb91-4978-96ce-da9a65d8c7dc",
        "showInput": false
      },
      "source": [
        "2. We can now load the train and test dataset using `ShakespeareDataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "executionStartTime": 1636676291516,
        "executionStopTime": 1636676292257,
        "hidden_ranges": [],
        "id": "tfRiZPphsQ79",
        "originalKey": "4f2e40f3-8e8d-4590-803a-6210cd4455c4",
        "requestMsgId": "5b6ad058-f5e0-4616-a05c-b9c1ec19fb01",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "# 2. Load the train and test datasets.\n",
        "train_dataset = ShakespeareDataset(\n",
        "    data_root=TRAIN_DATA,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = ShakespeareDataset(\n",
        "    data_root=TEST_DATA,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "iHBiHkf3sQ7-",
        "originalKey": "63946b49-8f29-473a-a694-4aa68e982593",
        "showInput": false
      },
      "source": [
        "Recall our `EXAMPLE_USER` from earlier? Their data now looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676292269,
        "executionStopTime": 1636676292273,
        "hidden_ranges": [],
        "id": "T3bVCtR4sQ7-",
        "originalKey": "8c2b0f96-2836-42b5-9d3b-cc468fb20a7a",
        "outputId": "788552f0-9541-4cf5-dfe6-8aa76350aef7",
        "requestMsgId": "0ecb6209-5c9c-45a3-adb7-1abb17865411",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[43, 14, 27,  ..., 12, 17, 24],\n",
            "        [14, 27, 14,  ..., 17, 24, 18],\n",
            "        [27, 14, 73,  ..., 24, 18, 12],\n",
            "        ...,\n",
            "        [18, 23, 29,  ..., 29, 27, 14],\n",
            "        [18, 23, 29,  ..., 29, 27, 14],\n",
            "        [23, 29, 17,  ..., 27, 14, 29]]), [32, 17, 18, 12, 17, 94, 34, 24, 30, 27, 94, 43, 18, 16, 17, 23, 14, 28, 28, 94, 32, 18, 21, 21, 94, 28, 14, 14, 94, 15, 18, 27, 28, 29, 75, 94, 84, 42, 18, 31, 18, 23, 16, 94, 10, 94, 25, 10, 25, 14, 27, 86, 94, 36, 94, 25, 21, 10, 34, 94, 29, 17, 14, 27, 14, 94, 18, 28, 73, 94, 22, 34, 94, 21, 24, 27, 13, 73, 94, 28, 24, 22, 14, 94, 29, 14, 23, 94, 32, 24, 27, 13, 28, 94, 21, 24, 23, 16, 73, 94, 58, 17, 18, 12, 17, 94, 18, 28, 94, 10, 28, 94, 11, 27, 18, 14, 15, 94, 10, 28, 94, 44, 94, 17, 10, 31, 14, 94, 20, 23, 24, 32, 23, 94, 10, 94, 25, 21, 10, 34, 78, 94, 37, 30, 29, 94, 11, 34, 94, 29, 14, 23, 94, 32, 24, 27, 13, 28, 73, 94, 22, 34, 94, 21, 24, 27, 13, 73, 94, 18, 29, 94, 18, 28, 94, 29, 24, 24, 94, 21, 24, 23, 16, 73, 94, 58, 17, 18, 12, 17, 94, 22, 10, 20, 14, 28, 94, 18, 29, 94, 29, 14, 13, 18, 24, 30, 28, 78, 94, 15, 24, 27, 94, 18, 23, 94, 10, 21, 21, 94, 29, 17, 14, 94, 25, 21, 10, 34, 94, 55, 17, 14, 27, 14, 94, 18, 28, 94, 23, 24, 29, 94, 24, 23, 14, 94, 32, 24, 27, 13, 94, 10, 25, 29, 73, 94, 24, 23, 14, 94, 25, 21, 10, 34, 14, 27, 94, 15, 18, 29, 29, 14, 13, 75, 94, 36, 23, 13, 94, 29, 27, 10, 16, 18, 12, 10, 21, 73, 94, 22, 34, 94, 23, 24, 11, 21, 14, 94, 21, 24, 27, 13, 73, 94, 18, 29, 94, 18, 28, 78, 94, 41, 24, 27, 94, 51, 34, 27, 10, 22, 30, 28, 94, 29, 17, 14, 27, 14, 18, 23, 94, 13, 24, 29, 17, 94, 20, 18, 21, 21, 94, 17, 18, 22, 28, 14, 21, 15, 75, 94, 58, 17, 18, 12, 17, 94, 32, 17, 14, 23, 94, 44, 94, 28, 10, 32, 94, 27, 14, 17, 14, 10, 27, 28, 68, 13, 73, 94, 44, 94, 22, 30, 28, 29, 94, 12, 24, 23, 15, 14, 28, 28, 73, 94, 48, 10, 13, 14, 94, 22, 18, 23, 14, 94, 14, 34, 14, 28, 94, 32, 10, 29, 14, 27, 78, 94, 11, 30, 29, 94, 22, 24, 27, 14, 94, 22, 14, 27, 27, 34, 94, 29, 14, 10, 27, 28, 94, 55, 17, 14, 94, 25, 10, 28, 28, 18, 24, 23, 94, 24, 15, 94, 21, 24, 30, 13, 94, 21, 10, 30, 16, 17, 29, 14, 27, 94, 23, 14, 31, 14, 27, 94, 28, 17, 14, 13, 75, 94, 43, 10, 27, 13, 74, 17, 10, 23, 13, 14, 13, 94, 22, 14, 23, 94, 29, 17, 10, 29, 94, 32, 24, 27, 20, 94, 18, 23, 94, 36, 29, 17, 14, 23, 28, 94, 17, 14, 27, 14, 73, 94, 58, 17, 18, 12, 17, 94, 23, 14, 31, 14, 27, 94, 21, 10, 11, 24, 30, 27, 68, 13, 94, 18, 23, 94, 29, 17, 14, 18, 27, 94, 22, 18, 23, 13, 28, 94, 29, 18, 21, 21, 94, 23, 24, 32, 78, 94, 36, 23, 13, 94, 23, 24, 32, 94, 17, 10, 31, 14, 94, 29, 24, 18, 21, 68, 13, 94, 29, 17, 14, 18, 27, 94, 30, 23, 11, 27, 14, 10, 29, 17, 14, 13, 94, 22, 14, 22, 24, 27, 18, 14, 28, 94, 58, 18, 29, 17, 94, 29, 17, 18, 28, 94, 28, 10, 22, 14, 94, 25, 21, 10, 34, 94, 10, 16, 10, 18, 23, 28, 29, 94, 34, 24, 30, 27, 94, 23, 30, 25, 29, 18, 10, 21, 75, 94, 49, 24, 73, 94, 22, 34, 94, 23, 24, 11, 21, 14, 94, 21, 24, 27, 13, 73, 94, 44, 29, 94, 18, 28, 94, 23, 24, 29, 94, 15, 24, 27, 94, 34, 24, 30, 75, 94, 44, 94, 17, 10, 31, 14, 94, 17, 14, 10, 27, 13, 94, 18, 29, 94, 24, 31, 14, 27, 73, 94, 36, 23, 13, 94, 18, 29, 94, 18, 28, 94, 23, 24, 29, 17, 18, 23, 16, 73, 94, 23, 24, 29, 17, 18, 23, 16, 94, 18, 23, 94, 29, 17, 14, 94, 32, 24, 27, 21, 13, 78, 94, 56, 23, 21, 14, 28, 28, 94, 34, 24, 30, 94, 12, 10, 23, 94, 15, 18, 23, 13, 94, 28, 25, 24, 27, 29, 94, 18, 23, 94, 29, 17, 14, 18, 27, 94, 18, 23, 29, 14, 23, 29, 28, 73, 94, 40, 33, 29, 27, 14, 22, 14, 21, 34, 94, 28, 29, 27, 14, 29, 12, 17, 68, 13, 94, 10, 23, 13, 94])\n"
          ]
        }
      ],
      "source": [
        "for user in train_dataset:\n",
        "  print(user)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "6xoU8ZcwsQ7-",
        "originalKey": "dd5a34ad-bf41-4b6b-82db-8dc528131310",
        "showInput": false
      },
      "source": [
        "To complete our data pipeline, we only need to\n",
        "\n",
        "3. Create a data loader, which will batchify training, eval, and test data. There is no need to create a sharder since each data sample is already associated with a user. For each dataset, the data loader splits each client's data into batches of size `batch_size`. We choose not to drop the last batch.\n",
        "\n",
        "4. Lastly, wrap the data loader with a data provider and return it. \n",
        "The data provider creates clients from the groupings in the data loader and adds metadata (e.g. number of examples, number of batches per client). \n",
        "Our data is now formatted such that the trainer will accept it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676292361,
        "executionStopTime": 1636676295007,
        "hidden_ranges": [],
        "id": "yHYYAP_ksQ7-",
        "originalKey": "09659734-9020-4f0e-a01f-4491a8f700c8",
        "outputId": "437ab223-383d-4d50-9251-bf9ab4e830d6",
        "requestMsgId": "52557db1-39c7-4695-b7c6-f3ac73ad8a0c",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating FL User: 133user [00:04, 30.13user/s]\n",
            "Creating FL User: 133user [00:00, 1079.14user/s]\n",
            "Creating FL User: 133user [00:00, 446.02user/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Clients in total: 133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from flsim.utils.example_utils import LEAFDataLoader, DataProvider\n",
        "\n",
        "# 3. Batchify training, eval, and test data. Note that train_dataset is already sharded.\n",
        "dataloader = LEAFDataLoader(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    test_dataset,\n",
        "    batch_size=LOCAL_BATCH_SIZE,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# 4. Wrap the data loader with a data provider.\n",
        "data_provider = DataProvider(dataloader)\n",
        "print(f\"\\nClients in total: {data_provider.num_train_users()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "PNtWCdk1sQ7_",
        "originalKey": "85636a6d-4c4b-49ea-9e82-34b3a9ddcbb3",
        "showInput": false
      },
      "source": [
        "### 2. Create the model\n",
        "\n",
        "Now, let's see how we can create a model that is compatible with FL-training.\n",
        "\n",
        "1. First, we define a standard, non-FL sentiment classification PyTorch `nn.Module`. In this tutorial we use a simple char-LSTM with an embedding, LSTM, and linear layer.\n",
        "\n",
        "2. Create a `torch.device` and choose where the model will be allocated (CUDA or CPU).\n",
        "\n",
        "As with the data pipeline, these steps are identical to creating a model in non-federated learning. Note that in contrast to non-FL learning, we haven't moved the model to device yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "executionStartTime": 1636676295011,
        "executionStopTime": 1636676295015,
        "hidden_ranges": [],
        "id": "fV3KjomGsQ7_",
        "originalKey": "257233d5-1f9f-47e3-8b1b-63ea4060e0c2",
        "requestMsgId": "e74219b0-4bc2-4520-ba01-e0650091ca80",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        n_hidden,\n",
        "        num_embeddings,\n",
        "        embedding_dim,\n",
        "        max_seq_len,\n",
        "        dropout_rate,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.n_hidden = n_hidden\n",
        "        self.num_classes = num_classes\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.num_embeddings, embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=self.n_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=self.dropout_rate,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.n_hidden, self.num_classes)\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_lens = torch.sum(x != (self.num_embeddings - 1), 1) - 1\n",
        "        x = self.embedding(x)  # [B, S] -> [B, S, E]\n",
        "        out, _ = self.lstm(x)  # [B, S, E] -> [B, S, H]\n",
        "        out = out[torch.arange(out.size(0)), seq_lens]\n",
        "        out = self.fc(self.dropout(out))  # [B, S, H] -> # [B, S, C]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "-aPFd8MisQ7_",
        "originalKey": "e78f06ab-88a9-44e9-b8f8-5c8ced763143",
        "showInput": false
      },
      "source": [
        "We initialize our model with such parameters that it is compatible with our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676295032,
        "executionStopTime": 1636676295038,
        "hidden_ranges": [],
        "id": "m-wCgAs-sQ8A",
        "originalKey": "f5a19f8d-34ca-4342-98e3-c68ea9ae7e0d",
        "outputId": "b61d1876-c6ba-4b9e-e79a-1c9d4640b117",
        "requestMsgId": "2f20621b-5f97-4396-adb5-e04ca2cebd92",
        "showInput": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(CharLSTM(\n",
              "   (embedding): Embedding(101, 100)\n",
              "   (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
              "   (fc): Linear(in_features=100, out_features=2, bias=True)\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              " ), device(type='cpu'))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Define our model, a simple char-LSTM.\n",
        "model = CharLSTM(\n",
        "    num_classes=train_dataset.num_classes,\n",
        "    n_hidden=100,\n",
        "    num_embeddings=train_dataset.num_letters + 1,\n",
        "    embedding_dim=100,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    dropout_rate=0.1,\n",
        ")\n",
        "\n",
        "# 2. Choose where the model will be allocated.\n",
        "cuda_enabled = torch.cuda.is_available() and USE_CUDA\n",
        "device = torch.device(f\"cuda:{0}\" if cuda_enabled else \"cpu\")\n",
        "\n",
        "model, device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PpvXknV-BTT"
      },
      "source": [
        "As with the data pipeline, there are a few extra steps that we need to take to make sure that our model is compatible with FL. In particular, we need to\n",
        "\n",
        "3. Wrap the PyTorch module with the FLSim `FLModel`, an abstracted version of a FL-friendly model class that is accepted by the trainer and handles metric collection, as well as the forward pass for both training and evaluation. We can recover our `nn.Module` by calling `FLModel.fl_get_module()`\n",
        "\n",
        "4. Move the model to GPU and enable CUDA if desired. `FLModel.fl_cuda()` internally calls `model.to(device)` to move the model to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bVqZB-sV9-lH"
      },
      "outputs": [],
      "source": [
        "from flsim.utils.example_utils import FLModel\n",
        "\n",
        "# 3. Wrap the model with FLModel.\n",
        "global_model = FLModel(model, device)\n",
        "assert(global_model.fl_get_module() == model)\n",
        "\n",
        "# 4. Move the model to GPU and enable CUDA if desired.\n",
        "if cuda_enabled:\n",
        "    global_model.fl_cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk5yZIbx9oBp"
      },
      "source": [
        "### 3. Metrics Reporting\n",
        "\n",
        "After having created our data pipeline and FL model, we will now create our metrics reporter. \n",
        "The metrics reporter allows us to collect, evaluate, and report relevant training, aggregation, and evaluation/test metrics as well as log them onto TensorBoard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "executionStartTime": 1636676328827,
        "executionStopTime": 1636676328872,
        "hidden_ranges": [],
        "id": "ePUW6EocsQ8B",
        "originalKey": "69332759-76d4-42b9-9de7-70333194b1b6",
        "requestMsgId": "5ee4134f-792d-42db-ba8d-a5864d511dfc",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "from flsim.interfaces.metrics_reporter import Channel\n",
        "from flsim.utils.example_utils import MetricsReporter\n",
        "\n",
        "# Create a metric reporter.\n",
        "metrics_reporter = MetricsReporter([Channel.TENSORBOARD, Channel.STDOUT])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgBTChrw-ZvA"
      },
      "source": [
        "There are three functions that are of particular interest:\n",
        "\n",
        "1. `compute_scores` computes the metrics of interest for both training and aggregation (if desired) as well as evaluation/test.\n",
        "\n",
        "2. `create_eval_metrics` creates a dictionary that stores the value for each eval metric. \n",
        "\n",
        "3. `compare_metrics` compares the current eval metrics that are returned by `create_eval_metrics` to the best eval metrics so far.\n",
        "\n",
        "For this tutorial, our only metric of interest is top-1 accuracy. In general, as with the data loading and model, you should write your own metrics reporter depending on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2jY4-_Rh-Zc2"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "if VERBOSE:\n",
        "    print(inspect.getsource(MetricsReporter.compute_scores))\n",
        "    print(inspect.getsource(MetricsReporter.create_eval_metrics))\n",
        "    print(inspect.getsource(MetricsReporter.compare_metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "customInput": null,
        "id": "kpYvMTiDsQ8A",
        "originalKey": "8e71e0e5-f0af-4d83-afa6-f079c131893a",
        "showInput": false
      },
      "source": [
        "### 4. Hyperparameters\n",
        "\n",
        "We can represent the hyperparameters for FL training in a JSON config for ease of representation and we convert the JSON config to OmegaConf before passing it to the FL trainer.\n",
        "\n",
        "In particular, we specify a FedAvg with LR implementation with 10 users per round."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676295041,
        "executionStopTime": 1636676295044,
        "hidden_ranges": [],
        "id": "Gdvpt6ppsQ8A",
        "originalKey": "3474bdbb-845c-4ad8-837d-e6561001c9ed",
        "outputId": "d6719f11-c741-42f5-dcbe-3c6aaca31411",
        "requestMsgId": "7ea697a0-0b6b-4d92-86ca-4b40f66915db",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  message=\"hydra.experimental.compose() is no longer experimental.\"\n"
          ]
        }
      ],
      "source": [
        "import flsim.configs\n",
        "from flsim.utils.config_utils import fl_config_from_json\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "json_config = {\n",
        "    \"trainer\": {\n",
        "        \"_base_\": \"base_sync_trainer\",\n",
        "        \"server\": {\n",
        "            \"_base_\": \"base_sync_server\",\n",
        "            \"server_optimizer\": {\n",
        "                # there are different types of server optimizers\n",
        "                # fed avg with lr requires a learning rate, whereas e.g. fed_avg doesn't\n",
        "                \"_base_\": \"base_fed_avg_with_lr\",\n",
        "                # server's learning rate\n",
        "                \"lr\": 0.7,\n",
        "                # server's global momentum\n",
        "                \"momentum\": 0.9\n",
        "            },\n",
        "            # aggregate client models into a single model by taking their weighted sum\n",
        "            \"aggregation_type\": \"WEIGHTED_AVERAGE\",\n",
        "            # type of user selection sampling\n",
        "            \"active_user_selector\": {\n",
        "                \"_base_\": \"base_uniformly_random_active_user_selector\"\n",
        "            }\n",
        "        },\n",
        "        \"client\": {\n",
        "            # number of client's local epochs\n",
        "            \"epochs\": 1,\n",
        "            \"optimizer\": {\n",
        "                # client's optimizer\n",
        "                \"_base_\": \"base_optimizer_sgd\",\n",
        "                # client's local learning rate\n",
        "                \"lr\": 1,\n",
        "                # client's local momentum\n",
        "                \"momentum\": 0\n",
        "            }\n",
        "        },\n",
        "        # number of users per round for aggregation\n",
        "        \"users_per_round\": 10,\n",
        "        # total number of global epochs\n",
        "        # total #rounds = ceil(total_users / users_per_round) * epochs\n",
        "        \"epochs\": 1,\n",
        "        # frequency of reporting train metrics\n",
        "        \"train_metrics_reported_per_epoch\": 10,\n",
        "        # keep the trained model always (as opposed to only when it\n",
        "        # performs better than the previous model on eval)\n",
        "        \"always_keep_trained_model\": False,\n",
        "        # frequency of evaluation per epoch\n",
        "        \"eval_epoch_frequency\": 1,\n",
        "        \"do_eval\": True,\n",
        "        # should we report train metrics after global aggregation\n",
        "        \"report_train_metrics_after_aggregation\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "cfg = fl_config_from_json(json_config)\n",
        "if VERBOSE: print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "m4Lbos5vsQ8B",
        "originalKey": "8ca763b7-c287-4a28-af1a-ca3b7ca5d3e4",
        "showInput": false
      },
      "source": [
        "### 5. Training\n",
        "Recall that we already built the data provider and created a model compatible with FL training. \n",
        "We also initialized a metrics reporter and set our desired hyperparameters.\n",
        "\n",
        "Now, we only need to instantiate the trainer with the model and hyperparameter config we defined earlier to launch the FL training flow. We run FL training with the above JSON config and utilize `eval_score` to store the final evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636676328880,
        "executionStopTime": 1636677467749,
        "hidden_ranges": [],
        "id": "VWBl_nidsQ8B",
        "originalKey": "9ada7326-5d23-419d-9f7c-58ad678f9f75",
        "outputId": "dd928325-2621-43ef-e1c1-b79abe6e3d90",
        "requestMsgId": "8e56e399-4b5c-4f0a-92fa-026d46e696cb",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  10%|█         | 59/588 [00:26<03:53,  2.26round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 59\n",
            "(epoch = 1, round = 59, global round = 59), Loss/Training: 0.8090709392856431\n",
            "(epoch = 1, round = 59, global round = 59), Accuracy/Training: 51.68040583386176\n",
            "reporting (epoch = 1, round = 59, global round = 59) for aggregation\n",
            "(epoch = 1, round = 59, global round = 59), Loss/Aggregation: 0.684047845005989\n",
            "(epoch = 1, round = 59, global round = 59), Accuracy/Aggregation: 61.111111111111114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  20%|██        | 118/588 [00:51<03:30,  2.24round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 118\n",
            "(epoch = 1, round = 118, global round = 118), Loss/Training: 0.7293585050800765\n",
            "(epoch = 1, round = 118, global round = 118), Accuracy/Training: 51.34589502018842\n",
            "reporting (epoch = 1, round = 118, global round = 118) for aggregation\n",
            "(epoch = 1, round = 118, global round = 118), Loss/Aggregation: 0.7340972363948822\n",
            "(epoch = 1, round = 118, global round = 118), Accuracy/Aggregation: 53.333333333333336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  30%|███       | 177/588 [01:15<02:54,  2.36round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 177\n",
            "(epoch = 1, round = 177, global round = 177), Loss/Training: 0.7436798909211038\n",
            "(epoch = 1, round = 177, global round = 177), Accuracy/Training: 49.735099337748345\n",
            "reporting (epoch = 1, round = 177, global round = 177) for aggregation\n",
            "(epoch = 1, round = 177, global round = 177), Loss/Aggregation: 0.8036821871995926\n",
            "(epoch = 1, round = 177, global round = 177), Accuracy/Aggregation: 58.064516129032256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  40%|████      | 236/588 [01:40<02:29,  2.36round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 236\n",
            "(epoch = 1, round = 236, global round = 236), Loss/Training: 0.7549871488647946\n",
            "(epoch = 1, round = 236, global round = 236), Accuracy/Training: 49.3844049247606\n",
            "reporting (epoch = 1, round = 236, global round = 236) for aggregation\n",
            "(epoch = 1, round = 236, global round = 236), Loss/Aggregation: 0.5968223094940186\n",
            "(epoch = 1, round = 236, global round = 236), Accuracy/Aggregation: 71.42857142857143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  50%|█████     | 294/588 [02:04<02:07,  2.30round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 294\n",
            "(epoch = 1, round = 294, global round = 294), Loss/Training: 0.809403382010899\n",
            "(epoch = 1, round = 294, global round = 294), Accuracy/Training: 49.89043097151205\n",
            "reporting (epoch = 1, round = 294, global round = 294) for aggregation\n",
            "(epoch = 1, round = 294, global round = 294), Loss/Aggregation: 0.8896376311779022\n",
            "(epoch = 1, round = 294, global round = 294), Accuracy/Aggregation: 46.666666666666664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  60%|██████    | 353/588 [02:28<01:38,  2.38round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 353\n",
            "(epoch = 1, round = 353, global round = 353), Loss/Training: 0.8145510965356266\n",
            "(epoch = 1, round = 353, global round = 353), Accuracy/Training: 53.088235294117645\n",
            "reporting (epoch = 1, round = 353, global round = 353) for aggregation\n",
            "(epoch = 1, round = 353, global round = 353), Loss/Aggregation: 0.7818179458379746\n",
            "(epoch = 1, round = 353, global round = 353), Accuracy/Aggregation: 40.74074074074074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  70%|███████   | 412/588 [02:52<01:15,  2.35round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 412\n",
            "(epoch = 1, round = 412, global round = 412), Loss/Training: 0.7623047745848065\n",
            "(epoch = 1, round = 412, global round = 412), Accuracy/Training: 50.319375443577\n",
            "reporting (epoch = 1, round = 412, global round = 412) for aggregation\n",
            "(epoch = 1, round = 412, global round = 412), Loss/Aggregation: 0.7443704962730407\n",
            "(epoch = 1, round = 412, global round = 412), Accuracy/Aggregation: 66.66666666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  80%|████████  | 471/588 [03:17<00:50,  2.32round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 471\n",
            "(epoch = 1, round = 471, global round = 471), Loss/Training: 0.8022697302820113\n",
            "(epoch = 1, round = 471, global round = 471), Accuracy/Training: 50.424328147100425\n",
            "reporting (epoch = 1, round = 471, global round = 471) for aggregation\n",
            "(epoch = 1, round = 471, global round = 471), Loss/Aggregation: 0.7024033606052399\n",
            "(epoch = 1, round = 471, global round = 471), Accuracy/Aggregation: 42.857142857142854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round:  90%|█████████ | 530/588 [03:41<00:25,  2.29round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 530\n",
            "(epoch = 1, round = 530, global round = 530), Loss/Training: 0.7606214851493368\n",
            "(epoch = 1, round = 530, global round = 530), Accuracy/Training: 51.7467248908297\n",
            "reporting (epoch = 1, round = 530, global round = 530) for aggregation\n",
            "(epoch = 1, round = 530, global round = 530), Loss/Aggregation: 0.5512452930212021\n",
            "(epoch = 1, round = 530, global round = 530), Accuracy/Aggregation: 69.23076923076923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round: 100%|█████████▉| 587/588 [04:06<00:00,  2.23round/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train finished Global Round: 588\n",
            "(epoch = 1, round = 588, global round = 588), Loss/Training: 0.7553911558512983\n",
            "(epoch = 1, round = 588, global round = 588), Accuracy/Training: 46.5288035450517\n",
            "reporting (epoch = 1, round = 588, global round = 588) for aggregation\n",
            "(epoch = 1, round = 588, global round = 588), Loss/Aggregation: 0.6767090618610382\n",
            "(epoch = 1, round = 588, global round = 588), Accuracy/Aggregation: 57.142857142857146\n",
            "Running (epoch = 1, round = 588, global round = 588) for Eval\n",
            "(epoch = 1, round = 588, global round = 588), Loss/Eval: 0.8684062315448778\n",
            "(epoch = 1, round = 588, global round = 588), Accuracy/Eval: 49.61439588688946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Round: 100%|█████████▉| 587/588 [04:08<00:00,  2.36round/s]\n",
            "Epoch:   0%|          | 0/1 [04:08<?, ?epoch/s]\n"
          ]
        }
      ],
      "source": [
        "from hydra.utils import instantiate\n",
        "\n",
        "# Instantiate the trainer.\n",
        "trainer = instantiate(cfg.trainer, model=global_model, cuda_enabled=cuda_enabled)   \n",
        "\n",
        "# Launch FL training.\n",
        "final_model, eval_score = trainer.train(\n",
        "    data_provider=data_provider,\n",
        "    metrics_reporter=metrics_reporter,\n",
        "    num_total_users=data_provider.num_train_users(),\n",
        "    distributed_world_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "customInput": null,
        "hidden_ranges": [],
        "id": "9d8XGCZHsQ8B",
        "originalKey": "945fc6b7-a479-4fa1-a75c-3cf241a3f245",
        "showInput": false
      },
      "source": [
        "After training finishes, we evaluate the model and report the accuracy on the test set before finishing this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "customInput": null,
        "executionStartTime": 1636677467822,
        "executionStopTime": 1636677479301,
        "hidden_ranges": [],
        "id": "Ad2U4OBBsQ8C",
        "originalKey": "87a50242-9574-41b3-bf39-6f296f44005e",
        "outputId": "4302fc54-80a5-4d02-e292-770dc97b6f22",
        "requestMsgId": "fb7b0794-1455-4805-9e97-c8649d0cb213",
        "showInput": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running (epoch = 1, round = 1, global round = 1) for Test\n",
            "(epoch = 1, round = 1, global round = 1), Loss/Test: 0.8683878255035598\n",
            "(epoch = 1, round = 1, global round = 1), Accuracy/Test: 49.61439588688946\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Accuracy': 49.61439588688946}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can now test our trained model.\n",
        "trainer.test(\n",
        "    data_provider=data_provider,\n",
        "    metrics_reporter=MetricsReporter([Channel.STDOUT]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "TASSRPrUsQ8C",
        "originalKey": "835d5964-8dbf-458a-8360-b8cd30f7a4c4",
        "showInput": false
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we first showed how to get and preprocess LEAF's Sent140 dataset. \n",
        "We then built a data provider by splitting each client's data into batches. \n",
        "We defined a simple char-LSTM as our model, wrapped it with a model compatible with FL training, and moved it to GPU. \n",
        "Lastly, we set the hyperparameters for FL training, launched the training flow, and evaluated our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "id": "jWEEDuFIsQ8C",
        "originalKey": "663d0a7d-dd3c-401f-8f8e-b1db9ffd3cce",
        "showInput": false
      },
      "source": [
        "### Additional resources\n",
        "\n",
        "- For a more in-depth understanding of this tutorial, check out [example_utils.py](https://github.com/facebookresearch/FLSim/blob/main/flsim/utils/example_utils.py) where we define the data loader, data provider, `FLModel`, and metrics reporter that we use in this tutorial.\n",
        "\n",
        "- [FLSim tutorials](https://github.com/facebookresearch/FLSim/tree/main/tutorials) - check out our other tutorial on image classification.\n",
        "\n",
        "- Kairouz et al. (2021): [Advances and Open Problems in Federated Learning](https://arxiv.org/pdf/1912.04977.pdf). As the title suggests, an in-depth overview of advances and open problems in FL.\n",
        "\n",
        "- If you're interested in federated learning with differential privacy, take a look at [Opacus](https://opacus.ai/), a library that enables training PyTorch models with differential privacy. \n",
        "You can find a blog post introducing Opacus [here](https://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "captumWidgetMessage": {},
    "colab": {
      "collapsed_sections": [],
      "name": "sent140_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.16 ('dima')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "last_base_url": "",
    "last_kernel_id": "",
    "last_msg_id": "",
    "last_server_session_id": "",
    "outputWidgetContext": {},
    "vscode": {
      "interpreter": {
        "hash": "d4aead3301b3da5006a74551c6653dbd35a399eb828877509ba26ae94e3b62e5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
